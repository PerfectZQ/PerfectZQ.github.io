---
layout: post
title: 教师家访质量分析
tag: Machine Learning
---

## 分析过程
1. 获取教师的家访数据，筛选出 FEELING，EXPERIENCE，FAMILY_SITUATION，STUDY_SITUATION，GROU_UP_SITUATION，PARENTS_ADVICE，ADVICE 共6个属性来进行本次分析。
2. 由于数据中会存在各种特殊字符(如换行符、转义符)，空值以及重复数据的情况，需要对数据进行预处理。
3. 读取数据，对数据的文本信息进行分词。分词采用的是IKAnalyse分词器，本身包含一部分中文常用词并且支持扩展。
4. 将分词后的数据通过TF-IDF转换成向量。TF-IDF的中心思想是：在所有文档中出现词频很高的词，其向量权重值低（物以稀为贵，大家都有的就便宜），在同一文档中出现词频高的词，其向量权重值高（文档特有的，可以代表本文档的分词）。
5. 对每个教师的家访反馈信息采用余弦计算向量相似度，具体比较方式是，将一条反馈信息(包含6个属性)视为一个文档，将一个老师的所有文档的所有属性进行两两比较，对于每一个属性都会获得一个相似度的数组，然后求其均值。此均值就代表当前属性的相似度。最终会获得一个6维的相似度向量，代表当前老师的家访可信度。
6. 对所有教师的可信度向量聚类，分四类。验证推论，并导出可用模型。因为聚类每一次运行的结果都可能不一样，需要对导出的模型进行评估。
7. 通过模型对其他教师进行预测分类。
8. 统计分析，前台展现。

## 代码实现
  * 用于分词的工具类

```java
package com.zq.wordcut;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.wltea.analyzer.lucene.IKAnalyzer;

import java.io.IOException;
import java.io.StringReader;

/**
 * Created by zhangqiang on 2017/5/9.
 */
public class IKAnalyseUtils {
    /**
     * 将文本切分成分词
     *
     * @param text
     * @return
     */
    public static String textSplit(String text) {
        Analyzer analyzer = null;
        StringReader reader = null;
        StringBuffer sb = new StringBuffer();
        try {
            analyzer = new IKAnalyzer(true);
            reader = new StringReader(text);
            TokenStream ts = analyzer.tokenStream("", reader);
            CharTermAttribute term = ts.getAttribute(CharTermAttribute.class);
            while (ts.incrementToken()) {
                sb.append(term.toString() + " ");
            }
            if (sb.length()   0) {
                sb.deleteCharAt(sb.length() - 1);
            }
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            analyzer.close();
            reader.close();
        }
        return sb.toString();
    }

    public static void main(String[] args) {

        System.out.println(textSplit(
                "老师辛苦了"
        ));
    }
}
```

  * 用与文本分析的类

```scala
package com.zq.visit

import com.google.gson.{Gson, JsonObject}
import com.zq.wordcut.IKAnalyseUtils
import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}
import org.apache.spark.ml.linalg.{Vector =  MLVector}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.{Vector =  MLlibVector}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.bson.Document

import scala.collection.mutable.ArrayBuffer

/**
  * Created by zhangqiang on 2017/5/9.
  * 进行文本分析的类，包括读取数据，将文本转换成向量并计算相似度
  */
object TextDifferenceAnalyzer {

  // 一条完整的用于计算的家访信息
  case class VisitDoc(userInfo: String, parentVisitFeeling: String, teacherVisitFeeling: String, familyEnvironment: String, studyEnvironment: String, growEnvironment: String, parentAdvice: String, feedbackAdvice: String)


  /**
    * 计算教师家访数据的相似度
    *
    * @param path         家访数据所在路径
    * @param sparkSession RDD[(String,Map[String, Double]))] <=  (教师的userInfo,列名:相似度)
    * @return
    */
  def getTeacherSimilarityRDD(path: String)(sparkSession: SparkSession): RDD[Document] = {

    import sparkSession.implicits._
    import com.zq.visit.implicits._
    
    val visitData = sparkSession.read.option("header", true).csv(path)

    var convertedVisitData = visitData.map { row = 
      // 学校编码
      val school_code = Option(row.getAs[String]("SCHOOL_CODE")).getOrElse("")
      // 学校姓名
      val school_name = Option(row.getAs[String]("SCHOOL_NAME")).getOrElse("")
      // 教职工号
      val teacher_idcard = Option(row.getAs[String]("TEACHER_IDCARD")).getOrElse("")
      // 教师姓名
      val teacher_name = Option(row.getAs[String]("TEACHER_NAME")).getOrElse("")
      // 家长感受
      val feeling = Option(row.getAs[String]("FEELING")).getOrElse("").replaceAllFuckingCharacters
      // 教师家访经验感受
      val experience = Option(row.getAs[String]("EXPERIENCE")).getOrElse("").replaceAllFuckingCharacters
      // 家庭环境
      val family_situation = Option(row.getAs[String]("FAMILY_SITUATION")).getOrElse("").replaceAllFuckingCharacters
      // 学习环境
      val study_situation = Option(row.getAs[String]("STUDY_SITUATION")).getOrElse("").replaceAllFuckingCharacters
      // 成长环境
      val grow_up_situation = Option(row.getAs[String]("GROU_UP_SITUATION")).getOrElse("").replaceAllFuckingCharacters
      // 家长建议
      val parents_advice = Option(row.getAs[String]("PARENTS_ADVICE")).getOrElse("").replaceAllFuckingCharacters
      // 回访建议
      val advice = Option(row.getAs[String]("ADVICE")).getOrElse("").replaceAllFuckingCharacters


      val userInfo =
        s"""{
           |  "school_code" : "${school_code}",
           |  "school_name" : "${school_name}",
           |  "teacher_idcard" : "${teacher_idcard}",
           |  "teacher_name" : "${teacher_name}"
           |}
           |""".stripMargin
      val parentVisitFeeling = IKAnalyseUtils.textSplit(feeling)
      val teacherVisitFeeling = IKAnalyseUtils.textSplit(experience)
      val familyEnvironment = IKAnalyseUtils.textSplit(family_situation)
      val studyEnvironment = IKAnalyseUtils.textSplit(study_situation)
      val growEnvironment = IKAnalyseUtils.textSplit(grow_up_situation)
      val parentAdvice = IKAnalyseUtils.textSplit(parents_advice)
      val feedbackAdvice = IKAnalyseUtils.textSplit(advice)
      VisitDoc(userInfo, parentVisitFeeling, teacherVisitFeeling, familyEnvironment, studyEnvironment, growEnvironment, parentAdvice, feedbackAdvice)
    }.toDF

    // 每一列训练一个单独的VSM模型，每一次循环将DataFrame中的一列转换成向量
    for (column <- convertedVisitData.columns if ("userInfo" != column)) {
      convertedVisitData = convertTextCol2Vector(convertedVisitData, column, column + "Features")
    }

    val similarityRDD = convertedVisitData.rdd
      .groupBy(row =  row.getAs[String]("userInfo"))
      .filter(_._2.size   1) // 过滤掉只有一条家访信息的老师
      .map {
      case (userInfo: String, rows: Iterable[Row]) = 

        val teacher = Document.parse(userInfo).get("teacher_name").toString

        println(s"当前${teacher}下，包含${rows.size}条家访信息")

        val beginTime = System.currentTimeMillis()

        val similarityMap = rows.headOption.getOrElse(Row("")).schema // 取列名
          .iterator
          .filter("userInfo" != _.name) // 除去userInfo
          .map { field = 
          val name = field.name
          val docsArr = rows.map { row =  row.getAs(name).asInstanceOf[MLVector] }.toArray

          val beginTime = System.currentTimeMillis()
          val similarity = computeDocSimilarity(docsArr)
          val endTime = System.currentTimeMillis()
          println(s"计算${teacher}老师的${name}字段耗时${endTime - beginTime}毫秒")

          (name, similarity)
        }.toMap

        val endTime = System.currentTimeMillis()
        println(s"计算${teacher}老师的相似度向量耗时${endTime - beginTime}毫秒")


        val similarityJson: JsonObject = new JsonObject
        similarityMap.foreach(x =  similarityJson.addProperty(x._1, x._2))
        val similarityJsonStr = new Gson().toJson(similarityJson)
        val json = s"""{'userInfo':'$userInfo','similarity':$similarityJsonStr}"""

        println(s"${teacher}老师的相似度信息 =  $json")

        Document.parse(json)
    }
    similarityRDD
  }

  import scala.collection.JavaConverters._

  /**
    * @param similarity
    * @return 用于预测的RDD (userInfo,MllibVector)
    */
  def convertSimilarityDoc2PredictVector(similarity: RDD[Document]): RDD[(String, MLlibVector)] = {
    similarity.map { document = 
      val userInfo = document.get("userInfo").toString
      val similarityDoc = document.get("similarity").asInstanceOf[Document]
      val arr: Array[Double] = similarityDoc.entrySet().iterator().asScala.map(_.getValue.asInstanceOf[Double]).toArray
      (userInfo, Vectors.dense(arr))
    }
  }

  /**
    * 用TF-IDF算法将文档中的一列转成向量格式
    *
    * @param data      分词后的家访数据 VisitDoc
    * @param inputCol  要转换的列名
    * @param outputCol 转换结果的列名
    * @return
    */
  private def convertTextCol2Vector(data: DataFrame, inputCol: String, outputCol: String): DataFrame = {
    //  A tokenizer that converts the input string to lowercase and then splits it by white spaces.
    val tokenizer = new Tokenizer().setInputCol(inputCol).setOutputCol("words")
    val wordsData = tokenizer.transform(data)
    /**
      * Maps a sequence of terms to their term frequencies using the hashing trick.
      * 用 hashing trick（散列方法） 来映射一系列项集的项的出现频率
      * Currently we use Austin Appleby's MurmurHash 3 algorithm (MurmurHash3_x86_32)
      * 目前我们用的是 Austin Appleby 的算法 MurmurHash 3
      * to calculate the hash code value for the term object.
      * 来计算每一个项的hash code值
      * Since a simple modulo is used to transform the hash function to a column index,
      * 自从通过简单取模运算把hash函数转换成一个列索引后
      * it is advisable to use a power of two as the numFeatures parameter;
      * 用2的若干次幂作为特征参数才变得可行
      * otherwise the features will not be mapped evenly to the columns.
      * 否则，特征无法均匀的映射各个列
      */
    val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures")
    val featuredData = hashingTF.transform(wordsData)
    //  Compute the Inverse Document Frequency (IDF) given a collection of documents.
    //  需要处理两边数据，第一遍计算IDF向量，第二遍通过IDF计算TF IDF
    val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")
    val idfModel = idf.fit(featuredData)
    val rescaledData = idfModel.transform(featuredData)
    rescaledData.drop(inputCol, "words", "rawFeatures").withColumnRenamed("features", outputCol)
  }

  import scala.util.control.Breaks._

  /**
    * 计算相似度的方式：
    *                                   v1 * v2
    * 文档两两相似 Sim(d1,d2) = cos = ——————————————
    *                                 |v1| * |v2|
    * v1代表文档d1的n维向量，v2代表d2的n维向量
    *
    * @param docsArr 文档集中的所有转化成Vector的文档的集合，对于只有一条的数据集合，默认相似度为0.0
    * @return 文档集的相似度
    */
  private def computeDocSimilarity(docsArr: Array[MLVector]): Double = {
    // 向量集合的大小
    val size: Long = docsArr.size
    if (size   1) {
      val vectorArr = docsArr.map(features =  features.toSparse)
      // 记录两两向量的相似度
      val cosinArr = new ArrayBuffer[Double]()
      // 将所有向量进行两两匹配
      for (i <- 0 until vectorArr.length - 1) {
        for (j <- i + 1 until vectorArr.length) {
          // 稀疏向量i非零值的下标
          val indices_i = vectorArr(i).indices
          // 稀疏向量j非零值的下标
          val indices_j = vectorArr(j).indices
          // 向量i数组（包含零值）
          val denseVector_i = vectorArr(i).toArray
          // 向量j数组（包含零值）
          val denseVector_j = vectorArr(j).toArray
          // 获取两个向量值均不为0的下标
          val sameIndexArr = new ArrayBuffer[Int]()
          for (index_i <- indices_i) {
            breakable {
              for (index_j <- indices_j) {
                if (index_i <= index_j) {
                  if (index_i == index_j)
                    sameIndexArr += index_i
                  break
                }
              }
            }
          }
          /** 计算相似度 */
          // 分子
          var numerator: Double = 0
          for (index <- sameIndexArr) numerator += denseVector_i(index) * denseVector_j(index)
          // 分母
          var denominator: Double = 0
          var wi: Double = 0
          var wj: Double = 0
          // 向量i的非零值构成的数组
          val values_i = vectorArr(i).values
          // 向量j的非零值构成的数组
          val values_j = vectorArr(j).values
          for (w <- values_i) wi += w * w
          for (w <- values_j) wj += w * w
          denominator = math.sqrt(wi * wj)
          val cosin = numerator / denominator // 相似度[0,1]
          cosinArr += cosin
        }
      }
      var sum: Double = 0
      for (cosin <- cosinArr) sum += cosin
      return sum / cosinArr.length
    } else {
      return 0.0
    }
  }
}
```

  * 用于隐式转换的类

```scala
package com.zq.visit

/**
  * Created by zhangqiang on 2017/5/16.
  *
  * 此对象包含本项目所有的隐式转换
  *
  */
object implicits {

  // 'implicit' modifier cannot be used for top-level objects
  implicit class OrdinaryString(str: String) {

    implicit def String2OrdinaryString(str: String): OrdinaryString = new OrdinaryString(str)

    def replaceAllFuckingCharacters(): String = {
      str.replaceAll("\"", "").replaceAll("\\\\", "")
    }
  }

}
```

  * 将转换后的向量保存到MongoDB中

```scala
package com.zq.apps

import com.mongodb.spark.{MongoConnector, MongoSpark}
import com.zq.visit.{CollectionDict, KMeansModelTool, TextDifferenceAnalyzer}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.apache.spark.mllib.linalg.{Vector =  MLlibVector}
import org.bson.Document
import com.mongodb.spark.config._
import com.zq.mongo.Mongo
import org.apache.spark.rdd.RDD

object SaveVector{

  def main(args: Array[String]): Unit = {

    // uri中必须写全database和collection，否则会出现
    // Missing collection name. Set via the 'spark.mongodb.output.uri' or 'spark.mongodb.output.collection' property
    val conf = new SparkConf()
      .set("spark.mongodb.input.uri", Mongo.properties("uri"))
      .set("spark.mongodb.output.uri", Mongo.properties("uri"))

    val sparkSession = SparkSession.builder().config(conf).appName("Save Vector").getOrCreate()

    val similarityDocRDD = TextDifferenceAnalyzer.getTeacherSimilarityRDD("file:///home/spark/realdata/*.csv")(sparkSession).cache()

    val writeVectorMap = new HashMap[String, String]
    writeVectorMap += ("collection" -  CollectionDict.VISIT_REAL_VECTOR)
    writeVectorMap += ("writeConcern.w" -  "majority")
    val writeVectorConfig = WriteConfig(writeVectorMap, Some(WriteConfig(sparkSession)))
    MongoSpark.save(similarityDocRDD, writeVectorConfig)
  }
}
```

  * 聚类分析

```scala
package com.zq.apps

import com.mongodb.spark.MongoSpark
import com.mongodb.spark.config.ReadConfig
import com.zq.mongo.Mongo
import com.zq.visit.CollectionDict
import org.apache.spark.SparkConf
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.SparkSession
import org.bson.Document

import scala.collection.mutable.HashMap

/**
  * Created by zhangqiang on 2017/5/22.
  */
object VisitRealDataKMeans extends App {

  val conf = new SparkConf()
    .set("spark.mongodb.input.uri", Mongo.properties("uri"))

  val sparkSession = SparkSession.builder.config(conf).appName("Visit Real Data KMeans").getOrCreate()
  val sparkContext = sparkSession.sparkContext

  val ReadVectorMap = new HashMap[String, String]
  ReadVectorMap += ("collection" -  CollectionDict.VISIT_REAL_VECTOR)
  ReadVectorMap += ("readPreference.name" -  "secondaryPreferred")
  ReadVectorMap += ("partitioner" -  "MongoShardedPartitioner")
  val readConfig = ReadConfig(ReadVectorMap, Some(ReadConfig(sparkSession)))
  val resultRDD = MongoSpark.load(sparkSession.sparkContext, readConfig)
  val (trainingData, predictData) = resultRDD.randomSplit(Array(0.3, 0.7))
  val data = trainingData.map { doc = 
    val similarity = doc.get("similarity").asInstanceOf[Document]
    Vectors.dense(similarity.entrySet.iterator.asScala.map(_.getValue.asInstanceOf[Double]).toArray)
  }

  val clusters: KMeansModel = KMeans.train(data, 4, 30)
  clusters.save(sparkContext,"hdfs:///model/kmeans_model")

}
```

  * 预测

```scala
package com.zq.apps

import com.mongodb.spark.{MongoConnector, MongoSpark}
import com.zq.visit.{CollectionDict, KMeansModelTool, TextDifferenceAnalyzer}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.apache.spark.mllib.linalg.{Vector =  MLlibVector}
import org.bson.Document
import com.mongodb.spark.config._
import com.zq.mongo.Mongo
import org.apache.spark.rdd.RDD

import scala.collection.mutable.HashMap

/**
  * Created by zhangqiang on 2017/5/10.
  */
object VisitDataPredict {

  def main(args: Array[String]): Unit = {

    // uri中必须写全database和collection，否则会出现
    // Missing collection name. Set via the 'spark.mongodb.output.uri' or 'spark.mongodb.output.collection' property
    val conf = new SparkConf()
      .set("spark.mongodb.input.uri", Mongo.properties("uri"))
      .set("spark.mongodb.output.uri", Mongo.properties("uri"))


    val sparkSession = SparkSession.builder().config(conf).appName("Teacher Visit Data Prediction").getOrCreate()


    val model = KMeansModel.load(sparkSession.sparkContext, "hdfs:///model/kmeans_model")
  
    val similarityDocRDD = TextDifferenceAnalyzer.getTeacherSimilarityRDD("hdfs:///spark/realdata/*.csv")(sparkSession).cache()

    val writeVectorMap = new HashMap[String, String]
    writeVectorMap += ("collection" -  CollectionDict.VISIT_REAL_VECTOR)
    writeVectorMap += ("writeConcern.w" -  "majority")
    val writeVectorConfig = WriteConfig(writeVectorMap, Some(WriteConfig(sparkSession)))
    MongoSpark.save(similarityDocRDD, writeVectorConfig)

    val vectorRDD = TextDifferenceAnalyzer.convertSimilarityDoc2PredictVector(similarityDocRDD)
    // 对教师家访的数据进行预测
    val documentRDD: RDD[Document] = vectorRDD.map {
      case (userInfo: String, vector: MLlibVector) = 
        val predict: Int = model.predict(vector)
        Document.parse(userInfo).append("predict", predict)
    }

    val writePredictMap = new HashMap[String, String]
    writePredictMap += ("collection" -  CollectionDict.VISIT_REAL_PREDICT)
    writePredictMap += ("writeConcern.w" -  "majority")
    val writePredictConfig = WriteConfig(writePredictMap, Some(WriteConfig(sparkSession)))
    MongoSpark.save(documentRDD, writePredictConfig)
  }
}
```