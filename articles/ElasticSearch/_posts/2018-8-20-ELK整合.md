---
layout: post
title: ELK整合
tag: ElasticSearch
---
## FileBeat

### 介绍
FileBeat 由两个重要的组件构成，`inputs`和`harvesters`，这两个组件从尾部读取数据，然后将数据发送到指定的`outputs`

#### harvesters
`harvesters`负责逐行读取单个文件的内容，并将读取的数据发送到`output`。每个文件都会启动一个`harvesters`，并由`harvesters`负责打开和关闭文件。由于文件描述符在`harvesters`运行时会一直保持在打开状态，因此，如果文件在被收集时被删除或者重命名，FileBeat 仍然会读取该文件，即在`harvesters`被关闭之前，磁盘上的空间仍然被`harvesters`占用着。默认情况下，FileBeat 会保持文件处于打开状态，直到达到`close_inactive`。

* 如果文件在被`harvesters`读取文件时删除，关闭文件处理程序才会释放底层资源。
* 关闭`harvesters`后，只有在`scan_frequency`结束后才会再次启动文件的收集。
* 如果在`harvesters`关闭时移动或移除文件，则不会继续收集文件。

使用`close_ *`配置选项控制`harvesters`何时关闭。

#### inputs
`input`负责管理`harvesters`并查找所有要读取的源。

例如输入类型时`log`，则`input`会查找磁盘上所有能匹配上的文件，并为每个文件启动`harvester`。

FileBeat 目前支持多种输入类型。每种输入类型都可以定义多次。日志输入检查每个文件以查看是否需要启动收集器，是否已经运行，或者是否可以忽略该文件（参考[ignore_older](https://www.elastic.co/guide/en/beats/FileBeat/current/FileBeat-input-log.html#FileBeat-input-log-ignore-older)）。如果自`harvesters`关闭后文件的大小发生变化，则只会收集新行。

FileBeat 会保存每个文件的状态，并经常将状态刷新到磁盘中的注册表文件。状态用于记住`harvester`正在读取的最后一个偏移量并确保发送所有日志行。如果无法访问`output`(Elasticsearch/Logstash)，FileBeat 会跟踪发送的最后一行，并在`output`再次可用时继续读取文件。在 FileBeat 运行时，状态信息也会保存在内存中。重新启动 FileBeat 时，会读取注册表文件的数据来重建状态，FileBeat 会在最后一个已知位置继续运行每个收集器。

对于每个`input`，FileBeat 保存它找到的每个文件的状态。由于可以重命名或移动文件，因此文件名和路径不足以标识文件。对于每个文件，FileBeat 存储唯一标识符以检测先前是否读取过文件。

FileBeat 保证事件将至少一次(At least once)传递到配置的`output`，并且不会丢失数据。 FileBeat 能够实现此行为，因为它将每个事件的传递状态存储在注册表文件中。在已定义的`output`被阻止且尚未确认所有事件的情况下，FileBeat 将继续尝试发送事件，直到`output`确认已收到事件。如果 FileBeat 在发送事件的过程中关闭，它不会等待`output`确认所有事件。重新启动 FileBeat 时，将再次发送所有已经发送到`output`但在 FileBeat 关闭之前未确认的事件，确保每个事件至少发送一次，但最终可能会将重复事件发送到输出。可以通过设置`shutdown_timeout`将 FileBeat 配置为在关闭之前等待特定时间。

> Note: 涉及日志轮换和旧文件的删除时，FileBeat 的至少一次交付保证有一个限制。如果日志文件写入磁盘的速度超过 FileBeat 可以处理的速度，或者在`output`不可用时删除文件，数据有可能会丢失。在 Linux 上，FileBeat 也可能因为 inode 重用而跳过行。

[FileBeat input 所有配置项介绍](https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html)

### Manage Multiline Message
FileBeat 默认是一行一行的处理日志的，但是对于类似 Java 异常栈这种多行的 message 怎么处理呢？这就需要配置`filebeat.yml`中的`multiline`去指出哪些行是属于同一事件。

> Note: Logstash 中使用 Logstash multiline codec 实现多行事件处理可能会导致流和损坏数据的混合。因此尽量在事件数据发送到 Logstash 之前先处理多行事件。

[examples of configuring multiline message](https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html)

* `multiline.pattern`: 正则表达式，用于匹配行
* `multiline.negate`: `true`或`false`，匹配或不匹配，默认是 false。
* `multiline.match`: `before`或者`after`。

例如：
```shell
# 将匹配的行添加到不匹配的行后面，例如：
# a
# b
# b
multiline.pattern: ^b
multiline.negate: false
multiline.match: after

# 将匹配的行添加到不匹配的行前面，例如：
# b
# b
# a
multiline.pattern: ^b
multiline.negate: false
multiline.match: before

# 将不匹配的行添加到匹配的行后面，例如：
# b
# a
# a
multiline.pattern: ^b
multiline.negate: true
multiline.match: after

# 将不匹配的行添加到匹配的行前面，例如：
# a
# a
# b
multiline.pattern: ^b
multiline.negate: true
multiline.match: before
```
### 配置 logstash output
如果要使用 Logstash 对 FileBeat 收集的数据执行其他处理，首先需要配置`filebeat.yml`中的`output.logstash`，并注释掉`output.elasticsearch`。
```shell
vim /etc/FileBeat/filebeat.yml
FileBeat.prospectors:
- type: log
  paths:
    - /var/lib/logstash-tutorial.log 
output.logstash:
  hosts: ["hostname:5044"]

# 注意需要将 output.elasticsearch 注释掉
```

发送到 Logstash 的每个事件都包含一些元数据字段(metadata)，这样就可以在 Logstash 中使用这些字段进行索引或者过滤。

除了元数据字段之外，如果还想添加额外的自定义的信息给`output`，可以使用`fields`，这样`output`就可以更方便的通过字段过滤或者区分日志。字段可以是标量值，数组，字典或这些的任何嵌套组合。 默认情况下，在此处指定的字段会被分组到`output`文档中`fields`的下一级。 要将自定义字段存储为顶级字段，需要将`fields_under_root`选项设置为`true`。 如果在常规配置中声明了重复字段，此处声明的值优先级更高。
                                   
```yaml
filebeat.prospectors:
- type: log
  enabled: true
  paths:
    - /var/log/hadoop-yarn/hadoop-cmf-yarn-NODEMANAGER-cdh1.log.out
  fields:
    topic_id: yarn_log
- type: log
  enabled: true
  paths:
    - /var/log/spark2/spark2-history-server-cdh1.log
  fields:
    topic_id: spark_log

output.logstash:
  hosts: ["cdh3:5044"]
```

FileBeat 发送给`output`的事件如下：
```shell
...
2018-08-23T11:23:25.014+0800    DEBUG   [publish]       pipeline/processor.go:275       Publish event: {
  "@timestamp": "2018-08-23T03:23:25.014Z",
  "@metadata": {
    "beat": "filebeat",
    "type": "doc",
    "version": "6.2.3"
  },
  "beat": {
    "name": "cdh1",
    "hostname": "cdh1",
    "version": "6.2.3"
  },
  "source": "/var/log/hadoop-yarn/hadoop-cmf-yarn-NODEMANAGER-cdh1.log.out",
  "offset": 15771466,
  "message": "2018-05-31 13:13:25,414 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService: org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit",
  "prospector": {
    "type": "log"
  },
  "fields": {
    "topic_id": "yarn_log"
  }
}
...
```

### 安装
注意安装换对应平台的 FileBeat，以防出现`can not exec bianary file`的异常

[FileBeat目录结构](https://www.elastic.co/guide/en/beats/FileBeat/current/directory-layout.html)

### 启动
```shell
# 运行 FileBeats
# -e 记录到stderr并禁用syslog/文件输出
# -c 指定配置文件
# -d 启用对指定选择器的调试
sudo nohup filebeat -e -c filebeat.yml -d "publish" > filebeat.log 2>&1 &

# FileBeat 会在注册表中存储每个文件收集的状态，
# 想强制 FileBeat 从日志的最开始重新读取，可以直接删除注册表文件
sudo rm data/registry
# deb/rpm 安装路径
sudo rm /var/lib/filebeat/registry

# 查看 filebeat 运行日志，发送的事件格式
tail -100f /var/log/filebeat/filebeat
```


## Logstash

[logstash 目录结构](https://www.elastic.co/guide/en/logstash/current/config-setting-files.html)

### input plugin - kafka
[kafka input plugin reference](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html)

从 kafka topic 中读取 events，logstash 作为消费者进行 group 的管理，并使用 kafka 默认的 offset 管理策略。

Logstash 实例默认使用一个逻辑组(`group_id => "logstash"`)来订阅 Kafka topic，使用者可以运行多个线程来提高读取吞吐量。或者，直接使用相同的`group_id`运行多个 Logstash 实例，以跨物理机分散负载。主题中的消息将分发到具有相同`group_id`的所有 Logstash 实例。

理想情况下，线程数应该与分区数量一样多以实现完美平衡 - 线程多于分区意味着某些线程将处于空闲状态

kafka 的 metadata：

* \[@metadata]\[kafka]\[topic]: Original Kafka topic from where the message was consumed.
* \[@metadata]\[kafka]\[consumer_group]: Consumer group
* \[@metadata]\[kafka]\[partition]: Partition info for this message.
* \[@metadata]\[kafka]\[offset]: Original record offset for this message.
* \[@metadata]\[kafka]\[key]: Record key, if any.
* \[@metadata]\[kafka]\[timestamp]: Timestamp when this message was received by the Kafka broker.

配置 kafka input:

```shell
input {
  kafka {
    id => "kafka_inuput_logstash_person"
    bootstrap_servers => "kafka:9999"
    client_id => "test"
    group_id => "test"
    topics => ["logstash_person"]
    auto_offset_reset => "latest"
    consumer_threads => 5
    decorate_events => true
    type => "person"
  }
  kafka {
    id => "kafka_inuput_logstash_cat"
    bootstrap_servers => "kafka:9999"
    client_id => "test"
    group_id => "test"
    topics => ["logstash_cat"]
    auto_offset_reset => "latest"
    consumer_threads => 5
    decorate_events => true
    type => "cat"
  }
}
filter {
  if [type] == "person" {
    grok {
      match => { "message" => "%{WORD:name} %{DATE:birthday} %{IP:ip}" }
      remove_field => "message"
    }
  }
  else if [type] == "cat" {
    grok {
      match => { "message" => "%{WORD:kind} %{WORD:master} %{NUMBER:age}" }
    }
    mutate {
      add_field => { "read_timestamp" => "%{@timestamp}" }
    }
  }
}
output {
  elasticsearch {
    hosts => ["http://es_node:9200"]
    index => "logstash-%{[type]}-%{+YYYY.MM.dd}"
    manage_template => false
  }
} 
```

* id: input plugin 的唯一标识符
* bootstrap_server: kafka 节点地址
* client_id: 发出请求时传递给服务器的id字符串，这样包含逻辑的应用程序可以跟踪请求的来源。
* group_id: 消费者 groupID
* topics: kafka topic，数组类型
* auto_offset_reset: 当 Kafka 中没有初始偏移量或偏移量超出范围时的策略。`earliest`: 从头开始消费；`latest`: 从最新的offset开始消费；`none`: 如果没有找到消费者组的先前偏移量，则向消费者抛出异常；`anything else`: 直接向消费者抛出异常。
* consumer_threads: 消费者线程数
* decorate_events: 此属性会将当前 topic、offset、group、partition 等信息也带到 message 中


### filter plugin - grok
grok 是一个可以将非结构化的日志解析成为结构化和可查询数据的 filter plugin。[grok filter plugin reference](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html)

语法`%{SYNTAX:SEMANTIC}`，其中`SYNTAX`是匹配内容的格式，`SEMANTIC`是唯一标识符，可以理解为`key`。

例如：

```shell
# 日志文件 http.log 格式如下
55.3.244.1 GET /index.html 15824 0.043

filter {
  grok {
      match => { "message" => "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" }
  }
}

# 解析后，event 将会添加一些额外的字段：
client: 55.3.244.1
method: GET
request: /index.html
bytes: 15824
duration: 0.043
```
grok 已经支持的 pattern 可以参考[https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns](https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns)

grok 是基于正则表达式的，因此任何正则表达式在 grok 当中也是有效的。使用的正则表达式库是 Oniguruma。可以去 [Oniguruma 官网](https://github.com/kkos/oniguruma/blob/master/doc/RE)查看所支持的正则表达式语法。

有时候 logstash 没有满足要求的 pattern，可以自定自己的正则表达式，然后为匹配项定义一个字段。

语法：`(?<field_name> pattern)`，注意不需要再加`%{}`了

例如：
```shell
filter {
  grok {
    patterns_dir => ["./patterns"]
    match => { "message" => "%{SYSLOGBASE} (?<id>[0-9A-F]{10,11}): %{GREEDYDATA:syslog_message}" }
  }
}
```

除此之外还可以定义一个 pattern 文件。
```shell
vim ./patterns/my_pattern
ID [0-9A-F]{10,11}

# 然后通过 patterns_dir 说明自定义的 pattern 文件所在文件夹路径
filter {
  grok {
    patterns_dir => ["./patterns"]
    match => { "message" => "%{SYSLOGBASE} %{ID:id}: %{GREEDYDATA:syslog_message}" }
  }
}
```

[测试 grok 语法正确性](http://grokdebug.herokuapp.com/)

grok pattern 可以将 string 转换成数字类型，但是目前只支持 int 和 float，语法：`%{NUMBER:num:int}`

### filter plugin - mutate
mutate 用于重命名，删除，替换和修改事件中的字段。[mutate filter plugin reference](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html)

### 在 Logstash 的 config 文件中访问 metadata
```shell
input {
  beats {
    port => 5044
  }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}" 
  }
}
```

### 在 Logstash 的 config 文件中访问 fields

```shell
input {
    beats {
        port => "5044"
    }
}

output {
    stdout { codec => rubydebug }
    kafka {
        topic_id => "%{[fields][topic_id]}"
        codec => plain {
            format => "%{message}"
            charset => "UTF-8"
        }
        bootstrap_servers => "kafka:9999"
    }
}
```

### 启动

```shell
# 测试配置文件
bin/logstash -f pipeline1.conf --config.test_and_exit

# 启动 logstash 
# --config.reload.automatic 不需要重启就可以加载被修改的配置文件
nohup bin/logstash -f pipeline1.conf --config.reload.automatic 2>&1 &

# 查看 logstash 运行日志，日志路径和格式可以通过 config/log4j2.properties 控制
tail -100f logs/logstash-plain.log

# 注意如果想同时启动多个 logstash 实例，需要修改 config/logstash.yml 中的 path.data
```
