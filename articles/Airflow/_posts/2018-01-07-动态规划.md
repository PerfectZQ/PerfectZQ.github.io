---
layout: post
title: Airflow
tag:  Airflow
---

## Install
* [Installation](https://airflow.apache.org/docs/stable/installation.html)
* [Airflow 1.10安装](https://zhuanlan.zhihu.com/p/47259326)

```shell
$ sudo pip install --upgrade pip
$ sudo pip install -U setuptools

$ export AIRFLOW_HOME=~/airflow
$ pip install apache-airflow
$ cd $PYTHON_HOME/lib/python2.7/sit-packages/airflow/bin
# 将 airflow 安装到刚刚设置的 AIRFLOW_HOME 目录下
$ ./airflow

$ ls -l ~/airflow
total 40
-rw-rw-r-- 1 zhangqiang zhangqiang 36475 Jun 28 18:35 airflow.cfg
drwxrwxr-x 3 zhangqiang zhangqiang    23 Jun 28 18:35 logs
-rw-rw-r-- 1 zhangqiang zhangqiang  2588 Jun 28 18:35 unittests.cfg

# Init db
$ vim $AIRFLOW_HOME/airflow.cfg
sql_alchemy_conn = mysql://airflow-username:airflow-password@localhost:3306/airflow

$ ./airflow initdb

# Start the web server, default port is 8080
$ ./airflow webserver -p 8080 > airflow-webserver.log 2>&1 & 

# Start the scheduler
$ ./airflow scheduler> airflow-scheduler.log 2>&1 & 

```

## Define DAGs
```shell
$ mkdir $AIRFLOW_HOME/dags

# 定义 DAG
$ vim $AIRFLOW_HOME/dags/tutorial.py
#!/usr/bin/python
# -*- coding: utf-8 -*-
from datetime import timedelta
# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG
# Operators; we need this to operate!
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import days_ago

# These args will get passed on to each operator
# You can override them on a per-task basis during operator initialization
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(2),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
    # 'wait_for_downstream': False,
    # 'dag': dag,
    # 'sla': timedelta(hours=2),
    # 'execution_timeout': timedelta(seconds=300),
    # 'on_failure_callback': some_function,
    # 'on_success_callback': some_other_function,
    # 'on_retry_callback': another_function,
    # 'sla_miss_callback': yet_another_function,
    # 'trigger_rule': 'all_success'
}
dag = DAG(
    dag_id='tutorial',
    default_args=default_args,
    description='A simple tutorial DAG',
    schedule_interval=timedelta(days=1),
)

# t1, t2 and t3 are examples of tasks created by instantiating operators
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

t2 = BashOperator(
    task_id='sleep',
    depends_on_past=False,
    bash_command='sleep 5',
    retries=3,
    dag=dag,
)
dag.doc_md = __doc__

t1.doc_md = """\
#### Task Documentation
You can document your task using the attributes `doc_md` (markdown),
`doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets
rendered in the UI's Task Instance Details page.
![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)
"""
templated_command = """
{% for i in range(5) %}
    echo "{{ ds }}"
    echo "{{ macros.ds_add(ds, 7)}}"
    echo "{{ params.my_param }}"
{% endfor %}
"""

t3 = BashOperator(
    task_id='templated',
    depends_on_past=False,
    bash_command=templated_command,
    params={'my_param': 'Parameter I passed in'},
    dag=dag,
)

t1 >> [t2, t3]

# 检查是否有错误，如果命令行没有报错，就表示没太大问题。
$ python $AIRFLOW_HOME/dags/tutorial.py
# 查看生效的 DAGs
$ airflow list_dags -sd $AIRFLOW_HOME/dags
```


## Commands
```shell
# Usage
$ ./airflow -h
$ ./airflow <command> -h

# 测试任务 airflow test dag_id task_id execution_time
$ ./airflow test dag_id task_id2019-09-10

# 开始运行任务(这一步也可以在web界面点trigger按钮)
$ ./airflow trigger_dag test_task

# 守护进程运行webserver, 默认端口为8080，也可以通过`-p`来指定
$ ./airflow webserver -D  

# 守护进程运行调度器     
$ ./airflow scheduler -D   

# 守护进程运行调度器    
$ ./airflow worker -D          

# 暂停任务
$ ./airflow pause dag_id　     

# 取消暂停，等同于在web管理界面打开off按钮
$ ./airflow unpause dag_id     

# 查看task列表
$ ./airflow list_tasks dag_id

# 清空任务状态
$ ./airflow clear dag_id       

# 运行task
$ ./airflow run dag_id task_id execution_date
```