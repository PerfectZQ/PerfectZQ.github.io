---
layout: post
title: Airflow
tag:  Airflow
---

## Reference
* [如何部署一个健壮的 apache-airflow 调度系统](https://www.cnblogs.com/xiongnanbin/p/11836049.html)

## Components of Airflow
Airflow 有一些重要的组件

### Metastore Database
Contains information about the status of Tasks, DAGs, Variables, Connections, etc.

### WebServer
守护进程，使用 gunicorn 服务器（相当于 java 中的 tomcat ）处理并发请求，可通过修改`{AIRFLOW_HOME}/airflow.cfg`文件中`workers`的值来控制处理并发请求的进程数，`workers = 4`表示开启 4 个 gunicorn worker(进程)处理 web 请求。webserver 提供以下功能：
* 中止、恢复、触发任务。
* 监控正在运行的任务，断点续跑任务。
* 执行 ad-hoc 命令或 SQL 语句来查询任务的状态，日志等详细信息。
* 配置连接，包括不限于数据库、ssh 的连接等。

### Scheduler
守护进程，调度器，它周期性地轮询任务的调度计划，以确定是否触发任务执行

### Executor
执行器，Airflow 本身是一个综合平台，它兼容多种组件，所以在使用的时候有多种方案可以选择。比如最关键的几个执行器:
* [Debug Executor](https://airflow.apache.org/docs/1.10.10/executor/debug.html#debug-executor): 单进程顺序执行任务，默认执行器，通常只用于测试
* [Celery Executor](https://airflow.apache.org/docs/1.10.10/executor/celery.html#celery-executor): 分布式调度任务，生产环境常用。[Celery](https://docs.celeryproject.org/en/stable/getting-started/first-steps-with-celery.html) 是一个借助队列机制实现的分布式任务调度框架，它本身无队列功能，需要借助第三方组件，比如 Redis 或者 RabbitMQ。
    * Celery 的任务队列包含两个重要的组件
        1. [Broker](https://docs.celeryproject.org/en/stable/getting-started/first-steps-with-celery.html#id3): 存储要执行的命令列表，需要借助第三方的用于收发消息的消息中间件(Message Broker)，如 RabbitMQ、Redis
        2. [Result Backend](https://docs.celeryproject.org/en/stable/getting-started/first-steps-with-celery.html#keeping-results): 存储已完成命令的状态，一般存储到 Database
    * 当调度器`executor = CeleryExecutor`时，包含两个重要的守护进程：
        1. Celery Worker: 守护进程，通过`airflow worker -D`启动一个或多个 Celery 的任务队列，负责执行具体的 DAG 任务，默认队列名为`default`
        2. Celery Flower: 守护进程，通过`airflow flower -D`启动，消息队列监控工具，用于监控 Celery 消息队列，默认的端口为`5555`，可以在浏览器地址栏中输入`http://127.0.0.1:5555`来访问
* [Dask Executor](https://airflow.apache.org/docs/1.10.10/executor/dask.html#dask-executor): 动态任务调度，主要用于数据分析
* [Kubernetes Executor](https://airflow.apache.org/docs/1.10.10/kubernetes.html)

## Common Concepts of Airflow
* [DAG](https://airflow.apache.org/docs/1.10.10/concepts.html#dags)：即有向无环图(Directed Acyclic Graph)，将所有需要运行的 Tasks 按照依赖关系组织起来，描述的是所有 Tasks 执行顺序。
* [Operator](https://airflow.apache.org/docs/1.10.10/concepts.html#operators)：可以简单理解为一个class，描述了 DAG 中某个的 task 具体要做的事。其中，airflow 内置了很多 operators，如 BashOperator 执行一个 bash 命令，PythonOperator 调用任意的 Python 函数，EmailOperator 用于发送邮件，HTTPOperator 用于发送HTTP请求， SqlOperator 用于执行SQL命令等等，同时，用户可以自定义 Operator，这给用户提供了极大的便利性。[Using Operators](https://airflow.apache.org/docs/1.10.10/howto/operator/index.html#using-operators)
* [Task](https://airflow.apache.org/docs/1.10.10/concepts.html#tasks)：Task 是 Operator 的一个实例，也就是 DAGs 中的一个 Node。
* [Task Instance](https://airflow.apache.org/docs/1.10.10/concepts.html#task-instances)：Task的一次运行。Web 界面中可以看到 Task Instance
* [Task Lifecycle](https://airflow.apache.org/docs/1.10.10/concepts.html#task-lifecycle)：Task从开始到完成整个生命周期各阶段的状态。
* [Task Relationship](https://airflow.apache.org/docs/1.10.10/concepts.html#relationship-builders)：DAGs 中的不同 Tasks 之间可以有依赖关系，如`Task1 >> Task2`，表明 Task2 依赖于 Task1 的结果。
* [Workflow](https://airflow.apache.org/docs/1.10.10/concepts.html#workflows)：通过将 DAGs 和 Operators 结合起来，用户就可以创建各种复杂的工作流(workflow)。

## Working Principle of Airflow
这里只说 CeleryExecutor 的工作原理。


## Install Airflow On ECS
* [Installation](https://airflow.apache.org/docs/stable/installation.html)
* [Airflow 1.10安装](https://zhuanlan.zhihu.com/p/47259326)

```shell
$ sudo pip install --upgrade pip
$ sudo pip install -U setuptools

$ export AIRFLOW_HOME=~/airflow
$ pip install apache-airflow
$ cd $PYTHON_HOME/lib/python2.7/sit-packages/airflow/bin
# 将 airflow 安装到刚刚设置的 AIRFLOW_HOME 目录下
$ ./airflow

$ cp -r $PYTHON_HOME/lib/python2.7/sit-packages/airflow/bin ~/airflow

$ ls -l ~/airflow
total 40
-rw-rw-r-- 1 zhangqiang zhangqiang 36475 Jun 28 18:35 airflow.cfg
drwxrwxr-x 3 zhangqiang zhangqiang    23 Jun 28 18:35 logs
-rw-rw-r-- 1 zhangqiang zhangqiang  2588 Jun 28 18:35 unittests.cfg


# 修改配置文件，所有节点公用一份
$ vim $AIRFLOW_HOME/airflow.cfg
# 配置 Metestore Database 
sql_alchemy_conn = mysql://{USERNAME}:{PASSWORD}@{MYSQL_HOST}:3306/airflow
# 配置 Broker - RabbitMQ
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#broker-settings
broker_url = amqp://{USERNAME}:{PASSWORD}@{RABBITMQ_HOST}:5672/
# 或者配置 Broker - Redis，这里指定使用 Redis db 0
# broker_url = redis://:{PASSWORD}@{REDIS_HOST}:6379/0
# 配置 Result Backend
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
result_backend = db+mysql://{USERNAME}:{PASSWORD}@{MYSQL_HOST}:3306/airflow

# 安装 mysql 依赖包，CentOS 
$ sudo yum install -y python-devel mysql-devel
# Ubuntu
$ sudo apt-get install -y python-dev python-MySQLdb
$ pip install 'apache-airflow[mysql]'

$ pip install 'apache-airflow[celery]'

$ ./airflow initdb

# Start the web server, default port is 8080
$ ./airflow webserver -D -p 8080

# Start the scheduler
$ ./airflow scheduler -D

# Start Celery Worker, 可以在多个节点运行多个 worker
$ ./airflow worker -D

```

## Install Airflow On Kubernetes
* [Install Airflow with Helm](https://hub.helm.sh/charts/bitnami/airflow)

```shell
# 创建 Secrets
$ kubectl create secret generic "airflow-mysql-password" --from-literal=mysql-password=123456
$ kubectl create secret generic "airflow-redis-password" --from-literal=redis-password=123456

$ helm repo add stable https://kubernetes-charts.storage.googleapis.com/
"stable" has been added to your repositories

$ helm search repo airflow
NAME            CHART VERSION   APP VERSION     DESCRIPTION
bitnami/airflow 6.3.5           1.10.10         Apache Airflow is a platform to programmaticall...
stable/airflow  7.2.0           1.10.10         Airflow is a platform to programmatically autho...

$ helm fetch stable/airflow

$ tar -zxvf airflow-7.2.0.tgz

$ cd airflow && ls -l
total 92
-rw-r--r-- 1 zhangqiang 197121   371  1月  1  1970 Chart.yaml
drwxr-xr-x 1 zhangqiang 197121     0  7月  8 19:56 charts
drwxr-xr-x 1 zhangqiang 197121     0  7月  8 19:56 examples
-rw-r--r-- 1 zhangqiang 197121    96  1月  1  1970 OWNERS
# 使用说明，可配置参数介绍
-rw-r--r-- 1 zhangqiang 197121 31708  1月  1  1970 README.md
-rw-r--r-- 1 zhangqiang 197121   332  1月  1  1970 requirements.lock
-rw-r--r-- 1 zhangqiang 197121   269  1月  1  1970 requirements.yaml
drwxr-xr-x 1 zhangqiang 197121     0  7月  8 19:56 templates
-rw-r--r-- 1 zhangqiang 197121  9417  1月  1  1970 UPGRADE.md
# 这里面包含所有可配置的属性，可以直接修改这里面的配置项目，而不是通过参数 + 模板的方式配置
-rw-r--r-- 1 zhangqiang 197121 33948  1月  1  1970 values.yaml

# 修改配置项
$ vim values.yml
###################################
# Airflow - Common Configs
###################################
airflow:
  ## configs for the docker image of the web/scheduler/worker
  ##
  image:
    repository: apache/airflow
    tag: 1.10.10-python3.6
    ## values: Always or IfNotPresent
    pullPolicy: IfNotPresent
    pullSecret: ""

  ## the airflow executor type to use
  ##
  ## NOTE:
  ## - this should be `CeleryExecutor` or `KubernetesExecutor`
  ## - if set to `KubernetesExecutor`:
  ##   - ensure that `workers.enabled` is `false`
  ##   - ensure that `flower.enabled` is `false`
  ##   - ensure that `redis.enabled` is `false`
  ##   - ensure that K8S configs are set in `airflow.config`
  ##   - we set these configs automatically:
  ##     - `AIRFLOW__KUBERNETES__NAMESPACE`
  ##     - `AIRFLOW__KUBERNETES__WORKER_SERVICE_ACCOUNT_NAME`
  ##     - `AIRFLOW__KUBERNETES__ENV_FROM_CONFIGMAP_REF`
  ##
  executor: CeleryExecutor

  ## the fernet key used to encrypt the connections/variables in the database
  ##
  ## WARNING:
  ## - you MUST customise this value, otherwise the encryption will be somewhat pointless
  ##
  ## NOTE:
  ## - to prevent this value being stored in your values.yaml (and airflow-env ConfigMap),
  ##   consider using `airflow.extraEnv` to define it from a pre-created secret
  ##
  ## GENERATE:
  ##   python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)"
  ##
  fernetKey: "7T512UXSSmBOkpWimFHIVb8jK6lfmSAvx4mO6Arehnc="

  ## environment variables for the web/scheduler/worker Pods (for airflow configs)
  ##
  ## WARNING:
  ## - don't include sensitive variables in here, instead make use of `airflow.extraEnv` with Secrets
  ## - don't specify `AIRFLOW__CORE__SQL_ALCHEMY_CONN`, `AIRFLOW__CELERY__RESULT_BACKEND`,
  ##   or `AIRFLOW__CELERY__BROKER_URL`, they are dynamically created from chart values
  ##
  ## NOTE:
  ## - airflow allows environment configs to be set as environment variables
  ## - they take the form: AIRFLOW__<section>__<key>
  ## - see the Airflow documentation: https://airflow.apache.org/docs/stable/howto/set-config.html
  ##
  ## EXAMPLE:
  ##   config:
  ##     ## Security
  ##     AIRFLOW__CORE__SECURE_MODE: "True"
  ##     AIRFLOW__API__AUTH_BACKEND: "airflow.api.auth.backend.deny_all"
  ##     AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"
  ##     AIRFLOW__WEBSERVER__RBAC: "False"
  ##
  ##     ## DAGS
  ##     AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "30"
  ##     AIRFLOW__CORE__LOAD_EXAMPLES: "False"
  ##
  ##     ## Email (SMTP)
  ##     AIRFLOW__EMAIL__EMAIL_BACKEND: "airflow.utils.email.send_email_smtp"
  ##     AIRFLOW__SMTP__SMTP_HOST: "smtpmail.example.com"
  ##     AIRFLOW__SMTP__SMTP_STARTTLS: "False"
  ##     AIRFLOW__SMTP__SMTP_SSL: "False"
  ##     AIRFLOW__SMTP__SMTP_PORT: "25"
  ##     AIRFLOW__SMTP__SMTP_MAIL_FROM: "admin@example.com"
  ##
  ##     ## Disable noisy "Handling signal: ttou" Gunicorn log messages
  ##     GUNICORN_CMD_ARGS: "--log-level WARNING"
  ##
  ##     ## Proxy Config
  ##     HTTP_PROXY: "http://proxy.example.com:8080"
  ##
  config: {}

  ## extra annotations for the web/scheduler/worker Pods
  ##
  ## EXAMPLE:
  ##   podAnnotations:
  ##     iam.amazonaws.com/role: airflow-Role
  ##
  podAnnotations: {}

  ## extra environment variables for the web/scheduler/worker (AND flower) Pods
  ##
  ## EXAMPLE:
  ##   extraEnv:
  ##     - name: AIRFLOW__CORE__FERNET_KEY
  ##       valueFrom:
  ##         secretKeyRef:
  ##           name: airflow-fernet-key
  ##           key: value
  ##     - name: AIRFLOW__LDAP__BIND_PASSWORD
  ##       valueFrom:
  ##         secretKeyRef:
  ##           name: airflow-ldap-password
  ##           key: value
  ##
  extraEnv: []

  ## extra configMap volumeMounts for the web/scheduler/worker Pods
  ##
  ## EXAMPLE:
  ##   extraConfigmapMounts:
  ##     - name: airflow-webserver-config
  ##       mountPath: /opt/airflow/webserver_config.py
  ##       configMap: airflow-webserver-config
  ##       readOnly: true
  ##       subPath: webserver_config.py
  ##
  extraConfigmapMounts: []

  ## extra containers for the web/scheduler/worker Pods
  ##
  ## EXAMPLE: (a sidecar that syncs DAGs from object storage)
  ##   extraContainers:
  ##     - name: s3-sync
  ##       image: my-user/s3sync:latest
  ##       volumeMounts:
  ##     - name: synchronised-dags
  ##       mountPath: /dags
  ##
  extraContainers: []

  ## extra pip packages to install in the web/scheduler/worker Pods
  ##
  ## EXAMPLE:
  ##   extraPipPackages:
  ##     - "airflow-exporter==1.3.1"
  ##
  extraPipPackages: []

  ## extra volumeMounts for the web/scheduler/worker Pods
  ##
  ## EXAMPLE:
  ##   extraVolumeMounts:
  ##     - name: synchronised-dags
  ##       mountPath: /opt/airflow/dags
  ##
  extraVolumeMounts: []

  ## extra volumes for the web/scheduler/worker Pods
  ##
  ## EXAMPLE:
  ##   extraVolumes:
  ##     - name: synchronised-dags
  ##       emptyDir: {}
  ##
  extraVolumes: []


###################################
# Airflow - Scheduler Configs
###################################
scheduler:
  ## resource requests/limits for the scheduler Pod
  ##
  ## EXAMPLE:
  ##   resources:
  ##     requests:
  ##       cpu: "1000m"
  ##       memory: "1Gi"
  ##
  #resources: {}
  resources:
    requests:
      cpu: "1000m"
      memory: "2Gi"


  ## the nodeSelector configs for the scheduler Pods
  ##
  nodeSelector: {}

  ## the affinity configs for the scheduler Pods
  ##
  #affinity: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - preference:
          matchExpressions:
          - key: project
            operator: In
            values:
            - dlink
        weight: 100

  ## the toleration configs for the scheduler Pods
  ##
  #tolerations: []
  tolerations:
  - effect: NoExecute
    key: project
    operator: Equal
    value: dlink

  ## labels for the scheduler Deployment
  ##
  labels: {}

  ## Pod labels for the scheduler Deployment
  ##
  podLabels: {}

  ## annotations for the scheduler Deployment
  ##
  annotations: {}

  ## Pod Annotations for the scheduler Deployment
  ##
  podAnnotations: {}

  ## configs for the PodDisruptionBudget of the scheduler
  ##
  podDisruptionBudget:
    ## if a PodDisruptionBudget resource is created for the scheduler
    ##
    enabled: true

    ## the maximum unavailable pods/percentage for the scheduler
    ##
    ## NOTE:
    ## - as there is only ever a single scheduler Pod,
    ##   this must be 100% for Kubernetes to be able to migrate it
    ##
    maxUnavailable: "100%"

    ## the minimum available pods/percentage for the scheduler
    ##
    minAvailable: ""

  ## custom airflow connections for the airflow scheduler
  ##
  ## NOTE:
  ## - connections are created with a script that is stored in a K8s secret and mounted into the scheduler container
  ##
  ## EXAMPLE:
  ##   connections:
  ##     - id: my_aws
  ##       type: aws
  ##       extra: |
  ##         {
  ##           "aws_access_key_id": "XXXXXXXXXXXXXXXXXXX",
  ##           "aws_secret_access_key": "XXXXXXXXXXXXXXX",
  ##           "region_name":"eu-central-1"
  ##         }
  ##
  connections: []

  ## custom airflow variables for the airflow scheduler
  ##
  ## NOTE:
  ## - THIS IS A STRING, containing a JSON object, with your variables in it
  ##
  ## EXAMPLE:
  ##   variables: |
  ##     { "environment": "dev" }
  ##
  variables: |
    {}

  ## custom airflow pools for the airflow scheduler
  ##
  ## NOTE:
  ## - THIS IS A STRING, containing a JSON object, with your pools in it
  ##
  ## EXAMPLE:
  ##   pools: |
  ##     {
  ##       "example": {
  ##         "description": "This is an example pool with 2 slots.",
  ##         "slots": 2
  ##       }
  ##     }
  ##
  pools: |
    {}

  ## the value of the `airflow --num_runs` parameter used to run the airflow scheduler
  ##
  ## NOTE:
  ## - this is the number of 'dag refreshes' before the airflow scheduler process will exit
  ## - if not set to `-1`, the scheduler Pod will restart regularly
  ## - for most environments, `-1` will be an acceptable value
  ##
  numRuns: -1

  ## if we run `airflow initdb` when the scheduler starts
  ##
  initdb: true

  ## if we run `airflow initdb` inside a special initContainer
  ##
  ## NOTE:
  ## - may be needed if you have custom database hooks configured that will be pulled in by git-sync
  ##
  preinitdb: false

  ## the number of seconds to wait (in bash) before starting the scheduler container
  ##
  initialStartupDelay: 0

  ## extra init containers to run before the scheduler Pod
  ##
  ## EXAMPLE:
  ##   extraInitContainers:
  ##     - name: volume-mount-hack
  ##       image: busybox
  ##       command: ["sh", "-c", "chown -R 1000:1000 logs"]
  ##       volumeMounts:
  ##         - mountPath: /opt/airflow/logs
  ##           name: logs-data
  ##
  extraInitContainers: []

###################################
# Airflow - WebUI Configs
###################################
web:
  ## resource requests/limits for the airflow web Pods
  ##
  ## EXAMPLE:
  ##   resources:
  ##     requests:
  ##       cpu: "500m"
  ##       memory: "1Gi"
  ##
  #resources: {}
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"

  ## the number of web Pods to run
  ##
  replicas: 1

  ## the nodeSelector configs for the web Pods
  ##
  nodeSelector: {}

  ## the affinity configs for the web Pods
  ##
  #affinity: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - preference:
          matchExpressions:
          - key: project
            operator: In
            values:
            - dlink
        weight: 100

  ## the toleration configs for the web Pods
  ##
  #tolerations: []
  tolerations:
  - effect: NoExecute
    key: project
    operator: Equal
    value: dlink

  ## labels for the web Deployment
  ##
  labels: {}

  ## Pod labels for the web Deployment
  ##
  podLabels: {}

  ## annotations for the web Deployment
  ##
  annotations: {}

  ## Pod annotations for the web Deployment
  ##
  podAnnotations: {}

  ## configs for the Service of the web Pods
  ##
  service:
    annotations: {}
    sessionAffinity: "None"
    sessionAffinityConfig: {}
    type: NodePort
    externalPort: 8080
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    nodePort:
      http: "49511"

  ## sets `AIRFLOW__WEBSERVER__BASE_URL`
  ##
  ## NOTE:
  ## - should be compatible with `ingress.web.path` config
  ##
  baseUrl: "http://localhost:8080"

  ## sets `AIRFLOW__CORE__STORE_SERIALIZED_DAGS`
  ##
  ## NOTE:
  ## - setting true will disable `git-sync` and `git-clone` containers in the web Pod
  ## - Docs: https://airflow.apache.org/docs/stable/dag-serialization.html
  ##
  serializeDAGs: false

  ## extra pip packages to install in the web container
  ##
  ## EXAMPLE: ( packages used by RBAC UI for OAuth )
  ##   extraPipPackages:
  ##     - "apache-airflow[google_auth]==1.10.10"
  ##
  extraPipPackages: []

  ## the number of seconds to wait (in bash) before starting the web container
  ##
  initialStartupDelay: 0

  ## the number of seconds to wait before declaring a new Pod available
  ##
  minReadySeconds: 5

  ## configs for the web Service readiness probe
  ##
  readinessProbe:
    enabled: false
    scheme: HTTP
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    successThreshold: 1
    failureThreshold: 3

  ## configs for the web Service liveness probe
  ##
  livenessProbe:
    enabled: true
    scheme: HTTP
    ## the number of seconds to wait before checking pod health
    ##
    ## NOTE:
    ## - make larger if you are installing many packages with:
    ##   `airflow.extraPipPackages`, `web.extraPipPackages`, or `dags.installRequirements`
    ##
    initialDelaySeconds: 300
    periodSeconds: 30
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 2

  ## the directory in which to mount secrets on web containers
  ##
  secretsDir: /var/airflow/secrets

  ## secret names which will be mounted as a file at `{web.secretsDir}/<secret_name>`
  ##
  ## EXAMPLE:
  ##   secrets:
  ##     - airflow-web-secret
  ##
  secrets: []

  ## you can use secretsMap to specify a map and all the secrets will be stored within it
  ## secrets will be mounted as files at `{web.secretsDir}/<secrets_in_map>`
  ## If you use web.secretsMap, then it overrides web.secrets.
  ##
  ## EXAMPLE:
  ## secretsMap: airflow-secrets
  ##
  secretsMap:

###################################
# Airflow - Worker Configs
###################################
workers:
  ## if the airflow workers StatefulSet should be deployed
  ##
  enabled: true

  ## resource requests/limits for the airflow worker Pods
  ##
  ## EXAMPLE:
  ##   resources:
  ##     requests:
  ##       cpu: "1000m"
  ##       memory: "2Gi"
  ##
  #resources: {}
  resources:
    requests:
      cpu: "1000m"
      memory: "2Gi"

  ## the number of workers Pods to run
  ##
  ## NOTE:
  ## - when `workers.autoscaling.enabled` is true, this is the minimum
  ##
  replicas: 1

  ## the nodeSelector configs for the worker Pods
  ##
  nodeSelector: {}

  ## the affinity configs for the worker Pods
  ##
  #affinity: {}
  affinity: 
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - preference:
          matchExpressions:
          - key: project
            operator: In
            values:
            - dlink
        weight: 100

  ## the toleration configs for the worker Pods
  ##
  #tolerations: []
  tolerations: 
  - effect: NoExecute
    key: project
    operator: Equal
    value: dlink

  ## labels for the worker StatefulSet
  ##
  labels: {}

  ## Pod labels for the worker StatefulSet
  ##
  podLabels: {}

  ## annotations for the worker StatefulSet
  ##
  annotations: {}

  ## Pod annotations for the worker StatefulSet
  ##
  podAnnotations: {}

  ## configs for the HorizontalPodAutoscaler of the worker Pods
  ##
  ## EXAMPLE:
  ##   autoscaling:
  ##     enabled: true
  ##     maxReplicas: 16
  ##     metrics:
  ##     - type: Resource
  ##       resource:
  ##         name: memory
  ##         target:
  ##           type: Utilization
  ##           averageUtilization: 80
  ##
  autoscaling:
    enabled: false
    maxReplicas: 2
    metrics: []

  ## the number of seconds to wait (in bash) before starting each worker container
  ##
  initialStartupDelay: 0

  ## configs for the celery worker Pods
  ##
  ## NOTE:
  ## - only takes effect if `airflow.executor` is `CeleryExecutor`
  ##
  celery:
    ## the number of tasks each celery worker can run at a time
    ##
    ## NOTE:
    ## - sets AIRFLOW__CELERY__WORKER_CONCURRENCY
    ##
    instances: 1

    ## if we should wait for tasks to finish on a celery worker before SIGTERM of Pod
    ##
    ## NOTE:
    ## - `workers.terminationPeriod` is still the overall timeout before worker Pods are killed using SIGKILL
    ##
    gracefullTermination: false

  ## how many seconds to wait for tasks on a worker to finish before SIGKILL
  ##
  terminationPeriod: 60

  ## directory in which to mount secrets on worker containers
  ##
  secretsDir: /var/airflow/secrets

  ## secret names which will be mounted as a file at `{workers.secretsDir}/<secret_name>`
  ##
  ## EXAMPLE:
  ##   secrets:
  ##     - airflow-worker-secret
  ##
  secrets: []

  ## you can use secretsMap to specify a map and all the secrets will be stored within it
  ## secrets will be mounted as files at `{workers.secretsDir}/<secrets_in_map>`
  ## If you use web.secretsMap, then it overrides workers.secrets.
  ##
  ## EXAMPLE:
  ## secretsMap: airflow-secrets
  ##
  secretsMap:

###################################
# Airflow - Flower Configs
###################################
flower:
  ## if the Flower UI should be deployed
  ##
  ## NOTE:
  ## - only takes effect if `airflow.executor` is `CeleryExecutor`
  ##
  enabled: true

  ## resource requests/limits for the flower Pods
  ##
  ## EXAMPLE:
  ##   resources:
  ##     requests:
  ##       cpu: "100m"
  ##       memory: "126Mi"
  ##
  #resources: {}
  resources:
    requests:
      cpu: "100m"
      memory: "126Mi"

  ## the nodeSelector configs for the flower Pods
  ##
  nodeSelector: {}

  ## the affinity configs for the flower Pods
  ##
  #affinity: {}
  affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - preference:
        matchExpressions:
        - key: project
          operator: In
          values:
          - dlink
      weight: 100

  ## the toleration configs for the flower Pods
  ##
  #tolerations: []
  tolerations:
  - effect: NoExecute
    key: project
    operator: Equal
    value: dlink

  ## labels for the flower Deployment
  ##
  labels: {}

  ## Pod labels for the flower Deployment
  ##
  podLabels: {}

  ## annotations for the flower Deployment
  ##
  annotations: {}

  ## Pod annotations for the flower Deployment
  ##
  podAnnotations: {}

  ## the name of a pre-created secret containing the basic authentication value for flower
  ##
  ## NOTE:
  ## - This sets `AIRFLOW__CELERY__FLOWER_BASIC_AUTH`
  ##
  basicAuthSecret: ""

  ## the key within `flower.basicAuthSecret` containing the basic authentication string
  ##
  basicAuthSecretKey: ""

  ## sets `AIRFLOW__CELERY__FLOWER_URL_PREFIX`
  ##
  ## NOTE:
  ## - should match `ingress.flower.path` config
  ##
  urlPrefix: ""

  ## configs for the Service of the flower Pods
  ##
  service:
    annotations: {}
    type: NodePort
    externalPort: 5555
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

  ## the number of seconds to wait (in bash) before starting the flower container
  ##
  initialStartupDelay: 0

  ## extra ConfigMaps to mount on the flower Pods
  ##
  ## EXAMPLE:
  ##   extraConfigmapMounts:
  ##     - name: extra-cert
  ##       mountPath: /etc/ssl/certs/extra-cert.pem
  ##       configMap: extra-certificates
  ##       readOnly: true
  ##       subPath: extra-cert.pem
  ##
  extraConfigmapMounts: []

###################################
# Airflow - Logs Configs
###################################
logs:
  ## the airflow logs folder
  ##
  path: /opt/airflow/logs

  ## configs for the logs PVC
  ##
  persistence:
    ## if a persistent volume is mounted at `logs.path`
    ##
    enabled: false

    ## the name of an existing PVC to use
    ##
    existingClaim: ""

    ## sub-path under `logs.persistence.existingClaim` to use
    ##
    subPath: ""

    ## the name of the StorageClass used by the PVC
    ##
    ## NOTE:
    ## - if set to "", then `PersistentVolumeClaim/spec.storageClassName` is omitted
    ## - if set to "-", then `PersistentVolumeClaim/spec.storageClassName` is set to ""
    ##
    storageClass: ""

    ## the access mode of the PVC
    ##
    ## WARNING:
    ## - must be: `ReadWriteMany`
    ##
    ## NOTE:
    ## - different StorageClass support different access modes:
    ##   https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
    ##
    accessMode: ReadWriteMany

    ## the size of PVC to request
    ##
    size: 1Gi

###################################
# Airflow - DAGs Configs
###################################
dags:
  ## the airflow dags folder
  ##
  path: /opt/airflow/dags

  ## whether to disable pickling dags from the scheduler to workers
  ##
  ## NOTE:
  ## - sets AIRFLOW__CORE__DONOT_PICKLE
  ##
  doNotPickle: false

  ## install any Python `requirements.txt` at the root of `dags.path` automatically
  ##
  ## WARNING:
  ## - if set to true, and you are using `dags.git.gitSync`, you must also enable
  ## `dags.initContainer` to ensure the requirements.txt is available at Pod start
  ##
  installRequirements: false

  ## configs for the dags PVC
  ##
  persistence:
    ## if a persistent volume is mounted at `dags.path`
    ##
    enabled: false

    ## the name of an existing PVC to use
    ##
    existingClaim: ""

    ## sub-path under `dags.persistence.existingClaim` to use
    ##
    subPath: ""

    ## the name of the StorageClass used by the PVC
    ##
    ## NOTE:
    ## - if set to "", then `PersistentVolumeClaim/spec.storageClassName` is omitted
    ## - if set to "-", then `PersistentVolumeClaim/spec.storageClassName` is set to ""
    ##
    storageClass: ""

    ## the access mode of the PVC
    ##
    ## WARNING:
    ## - must be one of: `ReadOnlyMany` or `ReadWriteMany`
    ##
    ## NOTE:
    ## - different StorageClass support different access modes:
    ##   https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
    ##
    accessMode: ReadOnlyMany

    ## the size of PVC to request
    ##
    size: 1Gi

  ## configs for the DAG git repository & sync container
  ##
  git:
    ## url of the git repository
    ##
    ## EXAMPLE: (HTTP)
    ##   url: "https://github.com/torvalds/linux.git"
    ##
    ## EXAMPLE: (SSH)
    ##   url: "ssh://git@github.com:torvalds/linux.git"
    ##
    url: ""

    ## the branch/tag/sha1 which we clone
    ##
    ref: master

    ## the name of a pre-created secret containing files for ~/.ssh/
    ##
    ## NOTE:
    ## - this is ONLY RELEVANT for SSH git repos
    ## - the secret commonly includes files: id_rsa, id_rsa.pub, known_hosts
    ## - known_hosts is NOT NEEDED if `git.sshKeyscan` is true
    ##
    secret: ""

    ## if we should implicitly trust [git.repoHost]:git.repoPort, by auto creating a ~/.ssh/known_hosts
    ##
    ## WARNING:
    ## - setting true will increase your vulnerability ot a repo spoofing attack
    ##
    ## NOTE:
    ## - this is ONLY RELEVANT for SSH git repos
    ## - this is not needed if known_hosts is provided in `git.secret`
    ## - git.repoHost and git.repoPort ARE REQUIRED for this to work
    ##
    sshKeyscan: false

    ## the name of the private key file in your `git.secret`
    ##
    ## NOTE:
    ## - this is ONLY RELEVANT for PRIVATE SSH git repos
    ##
    privateKeyName: id_rsa

    ## the host name of the git repo
    ##
    ## NOTE:
    ## - this is ONLY REQUIRED for SSH git repos
    ##
    ## EXAMPLE:
    ##   repoHost: "github.com"
    ##
    repoHost: ""

    ## the port of the git repo
    ##
    ## NOTE:
    ## - this is ONLY REQUIRED for SSH git repos
    ##
    repoPort: 22

    ## configs for the git-sync container
    ##
    gitSync:
      ## enable the git-sync sidecar container
      ##
      enabled: false

      ## resource requests/limits for the git-sync container
      ##
      ## NOTE:
      ## - when `workers.autoscaling` is true, YOU MUST SPECIFY a resource request
      ##
      ## EXAMPLE:
      ##   resources:
      ##     requests:
      ##       cpu: "50m"
      ##       memory: "64Mi"
      ##
      resources: {}

      ## the docker image for the git-sync container
      image:
        repository: alpine/git
        tag: latest
        ## values: Always or IfNotPresent
        pullPolicy: Always

      ## the git sync interval in seconds
      ##
      refreshTime: 60

  ## configs for the git-clone container
  ##
  ## NOTE:
  ## - use this container if you want to only clone the external git repo
  ##   at Pod start-time, and not keep it synchronised afterwards
  ##
  initContainer:
    ## enable the git-clone sidecar container
    ##
    ## NOTE:
    ## - this is NOT required for the git-sync sidecar to work
    ## - this is mostly used for when `dags.installRequirements` is true to ensure that
    ##   requirements.txt is available at Pod start
    ##
    enabled: false

    ## resource requests/limits for the git-clone container
    ##
    ## EXAMPLE:
    ##   resources:
    ##     requests:
    ##       cpu: "50m"
    ##       memory: "64Mi"
    ##
    resources: {}

    ## the docker image for the git-clone container
    image:
      repository: alpine/git
      tag: latest
      ## values: Always or IfNotPresent
      pullPolicy: Always

    ## path to mount dags-data volume to
    ##
    ## WARNING:
    ## - this path is also used by the git-sync container
    ##
    mountPath: "/dags"

    ## sub-path under `dags.initContainer.mountPath` to sync dags to
    ##
    ## WARNING:
    ## - this path is also used by the git-sync container
    ## - this MUST INCLUDE the leading /
    ##
    ## EXAMPLE:
    ##   syncSubPath: "/subdirWithDags"
    ##
    syncSubPath: ""

###################################
# Kubernetes - Ingress Configs
###################################
ingress:
  ## if we should deploy Ingress resources
  ##
  ## NOTE:
  ## - if you want to change url prefix for web ui or flower (even if you do not use this Ingress),
  ##   you can change `web.baseUrl` and `flower.urlPrefix`
  ##
  enabled: false

  ## configs for the Ingress of the web Service
  ##
  web:
    ## annotations for the web Ingress
    ##
    annotations: {}

    ## the path for the web Ingress
    ##
    ## WARNING:
    ## - do NOT include the trailing slash (for root, set an empty string)
    ##
    ## NOTE:
    ## - should be compatible with `web.baseUrl` config
    ##
    ## EXAMPLE: (if set to "/airflow")
    ## - UI:     http://example.com/airflow/admin
    ## - API:    http://example.com/airflow/api
    ## - HEALTH: http://example.com/airflow/health
    ##
    path: ""

    ## the hostname for the web Ingress
    ##
    host: ""

    ## the livenessPath for the web Ingress
    ##
    ## NOTE:
    ## - if set to "", defaults to: `{ingress.web.path}/health`
    ##
    livenessPath: ""

    ## configs for web Ingress TLS
    ##
    tls:
      ## enable TLS termination for the web Ingress
      ##
      enabled: false

      ## the name of a pre-created Secret containing a TLS private key and certificate
      ##
      ## NOTE:
      ## - this MUST be specified if `ingress.web.tls.enabled` is true
      ##
      secretName: ""

    ## http paths to add to the web Ingress before the default path
    ##
    ## EXAMPLE:
    ##   precedingPaths:
    ##     - path: "/*"
    ##       serviceName: "ssl-redirect"
    ##       servicePort: "use-annotation"
    ##
    precedingPaths: []

    ## http paths to add to the web Ingress after the default path
    ##
    ## EXAMPLE:
    ##   succeedingPaths:
    ##     - path: "/extra-service"
    ##       serviceName: "extra-service"
    ##       servicePort: "use-annotation"
    ##
    succeedingPaths: []

  ## configs for the Ingress of the flower Service
  ##
  flower:
    ## annotations for the flower Ingress
    ##
    annotations: {}

    ## the path for the flower Ingress
    ##
    ## WARNING:
    ## - do NOT include the trailing slash (for root, set an empty string)
    ##
    ## NOTE:
    ## - should match `flower.urlPrefix` config
    ##
    ## EXAMPLE: (if set to "/airflow/flower")
    ## - UI: http://example.com/airflow/flower
    ##
    path: ""

    ## the hostname for the flower Ingress
    ##
    host: ""

    ## the livenessPath for the flower Ingress
    ##
    ## WARNING:
    ## - keep the trailing slash
    ##
    ## NOTE:
    ## - if set to "", defaults to: `{ingress.flower.path}/`
    ##
    livenessPath: ""

    ## configs for flower Ingress TLS
    ##
    tls:
      ## enable TLS termination for the flower Ingress
      ##
      enabled: false

      ## the name of a pre-created Secret containing a TLS private key and certificate
      ##
      ## NOTE:
      ## - this MUST be specified if `ingress.flower.tls.enabled` is true
      ##
      secretName: ""

###################################
# Kubernetes - RBAC
###################################
rbac:
  ## if Kubernetes RBAC resources are created
  ##
  ## NOTE:
  ## - these allow the service account to create/delete Pods in the airflow namespace,
  ##   which is required for the KubernetesPodOperator() to function
  ##
  create: true

###################################
# Kubernetes - Service Account
###################################
serviceAccount:
  ## if a Kubernetes ServiceAccount is created
  ##
  ## NOTE:
  ## - if false, you must create the service account outside of this helm chart,
  ##   with the name: `serviceAccount.name`
  ##
  create: true

  ## the name of the ServiceAccount
  ##
  ## NOTE:
  ## - by default the name is generated using the `airflow.serviceAccountName` template in `_helpers.tpl`
  ##
  name: ""

  ## annotations for the ServiceAccount
  ##
  ## EXAMPLE: (to use WorkloadIdentity in Google Cloud)
  ##   annotations:
  ##     iam.gke.io/gcp-service-account: <<GCP_SERVICE>>@<<GCP_PROJECT>>.iam.gserviceaccount.com
  ##
  annotations: {}

###################################
# Kubernetes - Extra Manifests
###################################
## additional Kubernetes manifests to include with this chart
##
## EXAMPLE:
##   extraManifests:
##    - apiVersion: cloud.google.com/v1beta1
##      kind: BackendConfig
##      metadata:
##        name: "{{ .Release.Name }}-test"
##      spec:
##        securityPolicy:
##          name: "gcp-cloud-armor-policy-test"
##
extraManifests: []

###################################
# Database - PostgreSQL Chart
# - https://github.com/helm/charts/tree/master/stable/postgresql
###################################
postgresql:
  ## if the `stable/postgresql` chart is used
  ##
  ## WARNING:
  ## - this is NOT SUITABLE for production deployments of Airflow,
  ##   you should seriously consider using an external database service,
  ##   which can be configured with values under: `externalDatabase`
  ##
  ## NOTE:
  ## - set to `false` if using an external database
  ##
  enabled: false

  ## the postgres database to use
  ##
  postgresqlDatabase: airflow

  ## the postgres user to create
  ##
  postgresqlUsername: postgres

  ## the postgres user's password
  ##
  ## WARNING:
  ## - you should NOT use this, instead specify `postgresql.existingSecret`
  ##
  postgresqlPassword: airflow

  ## the name of a pre-created secret containing the postgres password
  ##
  existingSecret: ""

  ## the key within `postgresql.existingSecret` containing the password string
  ##
  existingSecretKey: "postgresql-password"

  ## configs for the PVC of postgresql
  ##
  persistence:
    ## if postgres will use Persistent Volume Claims to store data
    ##
    ## WARNING:
    ## - if false, data will be LOST as postgres Pods restart
    ##
    enabled: true

    ## the name of the StorageClass used by the PVC
    ##
    storageClass: ""

    ## the access modes of the PVC
    ##
    accessModes:
      - ReadWriteOnce

    ## the size of PVC to request
    ##
    size: 8Gi

###################################
# Database - External Database
# - these configs are only used when `postgresql.enabled` is false
###################################
externalDatabase:
  ## the type of external database: {mysql,postgres}
  ##
  type: mysql

  ## the host of the external database
  ##
  host: 10.10.41.147

  ## the port of the external database
  ##
  port: 3307

  ## the database/scheme to use within the the external database
  ##
  database: airflow

  ## the user of the external database
  ##
  user: airflow

  ## the name of a pre-created secret containing the external database password
  ##
  passwordSecret: "airflow-mysql-password"

  ## the key within `externalDatabase.passwordSecret` containing the password string
  ##
  passwordSecretKey: "mysql-password"

###################################
# Database - Redis Chart
# - https://github.com/helm/charts/tree/master/stable/redis
###################################
redis:
  ## if the `stable/redis` chart is used
  ##
  ## NOTE:
  ## - set to `false` if using an external redis database
  ## - set to `false` if `airflow.executor` is `KubernetesExecutor`
  ##
  enabled: false

  ## the redis password
  ##
  ## WARNING:
  ## - you should NOT use this, instead specify `redis.existingSecret`
  ##
  password: airflow

  ## the name of a pre-created secret containing the redis password
  ##
  existingSecret: ""

  ## the key within `redis.existingSecret` containing the password string
  ##
  existingSecretKey: "redis-password"

  ## configs for redis cluster mode
  ##
  cluster:
    ## if redis runs in cluster mode
    ##
    enabled: false

    ## the number of redis slaves
    ##
    slaveCount: 1

  ## configs for the redis master
  ##
  master:
    ## resource requests/limits for the master Pod
    ##
    ## EXAMPLE:
    ##   resources:
    ##     requests:
    ##       cpu: "100m"
    ##       memory: "256Mi"
    ##
    resources: {}

    ## configs for the PVC of the redis master
    ##
    persistence:
      ## use a PVC to persist data
      ##
      enabled: false

      ## the name of the StorageClass used by the PVC
      ##
      storageClass: ""

      ## the access mode of the PVC
      ##
      accessModes:
      - ReadWriteOnce

      ## the size of PVC to request
      ##
      size: 8Gi

  ## configs for the redis slaves
  ##
  slave:
    ## resource requests/limits for the slave Pods
    ##
    ## EXAMPLE:
    ##   resources:
    ##     requests:
    ##       cpu: "100m"
    ##       memory: "256Mi"
    ##
    resources: {}

    ## configs for the PVC of the redis slaves
    ##
    persistence:
      ## use a PVC to persist data
      ##
      enabled: false

      ## the name of the StorageClass used by the PVC
      ##
      storageClass: ""

      ## the access mode of the PVC
      ##
      accessModes:
        - ReadWriteOnce

      ## the size of PVC to request
      ##
      size: 8Gi

###################################
# Database - External Database
# - these configs are only used when `redis.enabled` is false
###################################
externalRedis:
  ## the host of the external redis
  ##
  host: 10.53.7.28

  ## the port of the external redis
  ##
  port: 6379

  ## the database number to use within the the external redis
  ##
  databaseNumber: 1

  ## the name of a pre-created secret containing the external redis password
  ##
  passwordSecret: "airflow-redis-password"

  ## the key within `externalRedis.passwordSecret` containing the password string
  ##
  passwordSecretKey: "redis-password"

###################################
# Prometheus - ServiceMonitor
###################################
serviceMonitor:
  ## if the ServiceMonitor resources should be deployed
  ##
  ## WARNING:
  ## - you will need an exporter in your airflow docker container, for example:
  ##   https://github.com/epoch8/airflow-exporter
  ##
  ## NOTE:
  ## - you can install pip packages with `airflow.extraPipPackages`
  ## - ServiceMonitor is a resource from: https://github.com/coreos/prometheus-operator
  ##
  enabled: false

  ## labels for ServiceMonitor, so that Prometheus can select it
  ##
  selector:
    prometheus: kube-prometheus

  ## the ServiceMonitor web endpoint path
  ##
  path: /admin/metrics

  ## the ServiceMonitor web endpoint interval
  ##
  interval: "30s"

###################################
# Prometheus - PrometheusRule
###################################
prometheusRule:
  ## if the PrometheusRule resources should be deployed
  ##
  ## WARNING:
  ## - you will need an exporter in your airflow docker container, for example:
  ##   https://github.com/epoch8/airflow-exporter
  ##
  ## NOTE:
  ## - you can install pip packages with `airflow.extraPipPackages`
  ## - PrometheusRule a resource from: https://github.com/coreos/prometheus-operator
  ##
  enabled: false

  ## labels for PrometheusRule, so that Prometheus can select it
  ##
  additionalLabels: {}

  ## alerting rules for Prometheus
  ##
  ## NOTE:
  ## - documentation: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
  ##
  groups: []



# 从本地安装，或者 helm install --name-template my-airflow -f values.yaml stable/airflow
$ helm install --name-template my-airflow -f values.yaml .

# 更新配置
$ helm upgrade my-airflow -f values.yaml .
Release "my-airflow" has been upgraded. Happy Helming!
NAME: my-airflow
LAST DEPLOYED: Thu Jul  9 22:51:51 2020
NAMESPACE: dlink-test
STATUS: deployed
REVISION: 3
TEST SUITE: None
NOTES:
Congratulations. You have just deployed Apache Airflow!

1. Get the Airflow Service URL by running these commands:
   export NODE_PORT=$(kubectl get --namespace dlink-test -o jsonpath="{.spec.ports[0].nodePort}" services my-airflow-web)
   export NODE_IP=$(kubectl get nodes --namespace dlink-test -o jsonpath="{.items[0].status.addresses[0].address}")
   echo http://$NODE_IP:$NODE_PORT/

2. Open Airflow in your web browser
```

## Define DAGs
```shell
$ mkdir $AIRFLOW_HOME/dags

# 定义 DAG
$ vim $AIRFLOW_HOME/dags/tutorial.py
#!/usr/bin/python
# -*- coding: utf-8 -*-
from datetime import timedelta
# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG
# Operators; we need this to operate!
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import days_ago

# These args will get passed on to each operator
# You can override them on a per-task basis during operator initialization
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(2),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
    # 'wait_for_downstream': False,
    # 'dag': dag,
    # 'sla': timedelta(hours=2),
    # 'execution_timeout': timedelta(seconds=300),
    # 'on_failure_callback': some_function,
    # 'on_success_callback': some_other_function,
    # 'on_retry_callback': another_function,
    # 'sla_miss_callback': yet_another_function,
    # 'trigger_rule': 'all_success'
}
dag = DAG(
    dag_id='tutorial',
    default_args=default_args,
    description='A simple tutorial DAG',
    schedule_interval=timedelta(days=1),
)

# t1, t2 and t3 are examples of tasks created by instantiating operators
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

t2 = BashOperator(
    task_id='sleep',
    depends_on_past=False,
    bash_command='sleep 5',
    retries=3,
    dag=dag,
)
dag.doc_md = __doc__

t1.doc_md = """\
#### Task Documentation
You can document your task using the attributes `doc_md` (markdown),
`doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets
rendered in the UI's Task Instance Details page.
![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)
"""
templated_command = """
{% for i in range(5) %}
    echo "{{ ds }}"
    echo "{{ macros.ds_add(ds, 7)}}"
    echo "{{ params.my_param }}"
{% endfor %}
"""

t3 = BashOperator(
    task_id='templated',
    depends_on_past=False,
    bash_command=templated_command,
    params={'my_param': 'Parameter I passed in'},
    dag=dag,
)

t1 >> [t2, t3]

# 检查是否有错误，如果命令行没有报错，就表示没太大问题。
$ python $AIRFLOW_HOME/dags/tutorial.py
# 查看生效的 DAGs
$ airflow list_dags -sd $AIRFLOW_HOME/dags
```


## Commands
```shell
# Usage
$ ./airflow -h
$ ./airflow <command> -h

# 测试任务 airflow test dag_id task_id execution_time
$ ./airflow test dag_id task_id2019-09-10

# 开始运行任务(这一步也可以在web界面点trigger按钮)
$ ./airflow trigger_dag test_task

# 守护进程运行webserver, 默认端口为8080，也可以通过`-p`来指定
$ ./airflow webserver -D  

# 守护进程运行调度器     
$ ./airflow scheduler -D   

# 守护进程运行调度器    
$ ./airflow worker -D          

# 暂停任务
$ ./airflow pause dag_id　     

# 取消暂停，等同于在web管理界面打开off按钮
$ ./airflow unpause dag_id     

# 查看task列表
$ ./airflow list_tasks dag_id

# 清空任务状态
$ ./airflow clear dag_id       

# 运行task
$ ./airflow run dag_id task_id execution_date
```