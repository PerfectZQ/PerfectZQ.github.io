---
layout: post
title: 集群搭建
tag: Spark
---

### Standalone模式部署
1. 去官网：http://spark.apache.org/downloads.html下载对应版本的Spark，解压。

2. 主节点设置：

    1) `vim /etc/hosts`
    ```shell
    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
    10.4.121.79 s12179
    10.4.121.80 s12180
    10.4.121.81 s12181
    10.4.121.91 s12191
    10.4.121.92 s12192
    ```
        
    2) `cd $SPARK_HOME/conf`
    
    3) `cp spark-env.sh.template spark-env.sh`
    
    4) `vi spark-env.sh`
    ```shell
    export SPARK_MASTER_HOST=s121202 # 前提是在hosts文件中指定了域名
    export JAVA_HOME=/usr/java/jdk1.8.0_121
    
    # Options for the daemons used in the standalone deploy mode
    # - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname
    # - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
    # - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
    # - SPARK_WORKER_CORES, to set the number of cores to use on this machine
    # - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
    # - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
    # - SPARK_WORKER_INSTANCES, to set the number of worker processes per node
    # - SPARK_WORKER_DIR, to set the working directory of worker processes
    # - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
    # - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).
    # - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
    # - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. "-Dx=y")
    # - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
    # - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers

    # Generic options for the daemons used in the standalone deploy mode
    # - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
    # - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
    # - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
    # - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
    # - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
    # - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.

    ```
    4) `vi ~/.bash_profile`
    ```shell
    SPARK_HOME=/opt/neu/spark-2.1.1-bin-hadoop2.6
    export SPARK_HOME
    PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
    export PATH
    ```
    5) `spark-shell` ，版本验证
    
    
    
3. 将从节点配置到slaves文件中

```shell
vi $SPARK_HOME/conf/slaves
# 添加下面的从节点
s121203
s121204
s121205
s121206
s121207
```

4. 配置Master无密钥登陆Slaves节点
```shell
    # 如果系统没有ssh，可能需要安装
    yum -y install openssh-server
    # 生成公钥 其中：-t 是类型 -P 是密码
    ssh-keygen -t rsa -P "" 
    # authorized_keys文件的权限为644
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    # 刷新
    sudo /etc/init.d/sshd reload
    # 记得将机器的防火墙关掉
    service iptables stop
    # ssh验证配置
    ssh localhost
    # 将Master节点的authorized_keys发送到所有的Slaves节点，并登陆验证
    scp ~/.ssh/authorized_keys neu@s121203:~/.ssh/
    
```

5. 将spark项目完整的发送到子节点
```shell
scp -rp /opt/neu/spark-2.1.1-bin-hadoop2.6 neu@s121203:/opt/neu/
```

6. 启动集群
```shell
cd $SPARK_HOME/
./sbin/start-all.sh
```

7. 查看启动情况
```shell
jps # 主节点可以看到Master进程，从节点可以看到Worker进程。如果主节点也配置在了slaves文件中，那么主节点能看到Master和Worker两个进程。
```