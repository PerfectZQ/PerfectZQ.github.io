---
layout: post
title: 异常处理记录
tag: Spark
---

## spark程序运行空指针异常的原因
1. 嵌套使用了RDD操作，比如在一个RDD map中又对另一个RDD进行了map操作。主要原因在于spark不支持RDD的嵌套操作。
2. 在RDD的算子操作中引用了object非原始类型(非int long等简单类型)的成员变量。是由于object的成员变量默认是无法序列化的。解决方法：可以先将成员变量赋值给一个临时变量，然后使用该临时变量即可
3. spark 2.0.0对kryo序列化的依赖有bug，到SPARK_HOME/conf/spark-defaults.conf
将默认： spark.serializer     org.apache.spark.serializer.KryoSerializer
改为： spark.serializer 	  org.apache.spark.serializer.JavaSerializer

## Task not serializable
参考:[https://stackoverflow.com/questions/28006517/redis-on-sparktask-not-serializable](https://stackoverflow.com/questions/28006517/redis-on-sparktask-not-serializable)

Spark划分好DAG图后，会把各个stage转换成taskSet，然后将各个task分发到不同的executor上去计算。分发时，task中包含的算子(窄依赖)例如`map()`中的所有引用对象，需要被序列化后才能发送到各executor。所以，在`map`等算子内部使用外部定义的函数和变量时，如果这个外部定义的变量不能被序列化，就会出现这种问题。

但是在算子的计算过程中难免需要使用外部变量，要使用这些外部变量，就需要对这些外部变量做好序列化工作。最普遍的情形是：当引用了某个类（绝大多数是当前class或者object）的某个成员变量或者成员函数的时候，将导致这个类的所有的成员都需要支持序列化。即便这个类extends了Serializable，声明支持序列化，如果某些字段(成员对象，Object)不支持序列化，仍然会导致这个类在序列化的时候出现问题。这种情况可以将类中不能被序列化的成员变量标注为`@transient`，表示不对其进行序列化。

解决Task not serializable的方法，按情况使用：

* 在条件允许的情况下，不要引用外部声明的对象，在map算子内部创建对象。
* 如果必须引用`map`算子外部的对象，那么就让这个对象在executor上初始化，即在local execution context中创建对象。例如创建一个redis的client，该对象就没有实现Serializable接口：

```scala
/**
 *  第一种：使用 rdd.mapPartitions
 *  一个partition就对应着一个task，一个分区中的数据共用一个对象
 */
resultRDD.mapPartitions {
  partition => {
    // create the connection in the context of the mapPartition operation
    val jedis = new Jedis("10.4.121.202", 6379)
    val res = partition.map {
      case (key: String, nameAndIP: String) =>
        jedis.hget(key, nameAndIP)
    }
    jedis.close
    res
  }
}
/**
 * 第二种：使用object，并声明对象为懒加载(lazy)
 * 这样在executor真正去执行的时候才会创建对象，相当于为每个
 * executor创建了一个连接对象(jedis)。
 */
object RedisUtil extends Serializable {
  // 使用lazy，当task被分发到executor上执行的时候才会创建对象，没有被创建的对象在分发时不需要被序列化。
  lazy val jedis: Jedis = new Jedis("10.4.121.202", 6379)
}
// main
object LogAnalyseAndStatistics {
  def main(args: Array[String]): Unit = {
    resultRDD = ...
    resultRDD.map {
        case (key: String, nameAndIP: String) =>
          // 相当于引用一个外部的实现了Serializable的RedisUtil的闭包
          RedisUtil.jedis.hget(key, nameAndIP)
    }
  }
}
```
* 同理。如果是引用了class的成员函数导致整个class需要支持序列化，可以通过将此成员函数放在Scala的object中，类似与Java的static，这样就取消了对类对象的依赖。
* 让不能被序列化的类extends Serializable接口，然后向KryoSerializer注册类，并声明该类的序列化方式。
```scala
SparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer")  
SparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable]))
```
* 如果不支持序列化的类不是你自定义的，是第三方包的类，可以将该类的对象放在广播变量中，广播变量会将该变量变成只读的变量，然后缓存在每一台机器(executor)节点上，而非每个Task保存一份拷贝，这样也可以避免Task not serializable的问题
```scala
val sparkContext = sparkSession.sparkContext
val jedisClusterNodes = new HashSet[HostAndPort]()
jedisClusterNodes.add(new HostAndPort("10.4.121.202", 6379))
...
val broadcastJedisCluster = sparkContext.broadcast(new JedisCluster(jedisClusterNodes))
```

## java.io.NotSerializableException: org.apache.spark.SparkContext
sparkContext、sparkConf、sparkSession都是不能被序列化的对象，所以他们不能出现在map等算子中。
## object not serializable(class:org.apache.hadoop.hbase.io.ImmutableBytesWritable)
Spark操作HBase返回的是`RDD[ImmutableWritable,Result]`类型，当在集群上对此RDD进行操作的时候。（比如`join`、`repartition`等进行shuffle），就会产生此异常，因为`org.apache.hadoop.hbase.io.ImmutableBytesWritable`和`org.apache.hadoop.hbase.client.Result`并没有实现`java.io.Serializable`接口

解决方式：

方式一：
```scala
// 向Spark注册，声明ImmutableWritable类需要使用KryoSerializer进行序列化
SparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer")  
SparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable]))
```
方式二：
将ImmutableWritable转换成其他可序列化的类，将其中的数据抽取出来放在可以序列化的类中，比如String或者数组

## java.lang.IllegalArgumentException: requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually org.apache.spark.mllib.linalg.VectorUDT@f71b0bce.
解决办法：
```scala
import org.apache.spark.mllib.linalg.Vectors
// 换成
import org.apache.spark.ml.linalg.Vectors
```
## java.lang.OutOfMemoryError: PermGen space

永生代内存溢出

解决方法：
在IDEA的run configuration中添加VM options  -XX:PermSize=128m

原因：
PermGen space用于存放Class和Meta的信息,Class在被 Load的时候被放入PermGen space区域，它和和存放Instance的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS的话,就很可能
出现PermGen space错误。这种错误常见在web服务器对JSP进行pre compile的时候。

改正方法：
```shell
-Xms256m -Xmx256m -XX:MaxNewSize=256m -XX:MaxPermSize=256m 
```

