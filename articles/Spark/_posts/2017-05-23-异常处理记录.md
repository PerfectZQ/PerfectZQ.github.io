---
layout: post
title: 异常处理记录
tag: Spark
---

## spark 运行空指针异常的原因
1. 嵌套使用了RDD操作，比如在一个RDD map中又对另一个RDD进行了map操作。主要原因在于spark不支持RDD的嵌套操作。
2. 在RDD的算子操作中引用了object非原始类型(非int long等简单类型)的成员变量。是由于object的成员变量默认是无法序列化的。解决方法：可以先将成员变量赋值给一个临时变量，然后使用该临时变量即可
3. spark 2.0.0对kryo序列化的依赖有bug，到SPARK_HOME/conf/spark-defaults.conf
将默认： spark.serializer     org.apache.spark.serializer.KryoSerializer
改为： spark.serializer 	  org.apache.spark.serializer.JavaSerializer

## Task not serializable
[https://stackoverflow.com/questions/28006517/redis-on-sparktask-not-serializable](https://stackoverflow.com/questions/28006517/redis-on-sparktask-not-serializable)

[https://stackoverflow.com/questions/22592811/task-not-serializable-java-io-notserializableexception-when-calling-function-ou](https://stackoverflow.com/questions/22592811/task-not-serializable-java-io-notserializableexception-when-calling-function-ou)

Spark划分好DAG图后，会把各个stage转换成taskSet，然后将各个task分发到不同的executor上去计算。分发时，task中包含的算子(窄依赖)例如`map()`中的所有引用对象，需要被序列化后才能发送到各executor。所以，在`map`等算子内部使用外部定义的函数和变量时，如果这个外部定义的变量不能被序列化，就会出现这种问题。

但是在算子的计算过程中难免需要使用外部变量，要使用这些外部变量，就需要对这些外部变量做好序列化工作。最普遍的情形是：当引用了某个类（绝大多数是当前class或者object）的某个成员变量或者成员函数的时候，将导致这个类的所有的成员都需要支持序列化。即便这个类extends了Serializable，声明支持序列化，如果某些字段(成员对象，Object)不支持序列化，仍然会导致这个类在序列化的时候出现问题。这种情况可以将类中不能被序列化的成员变量标注为`@transient`，表示不对其进行序列化。

解决Task not serializable的方法，按情况使用：

* 在条件允许的情况下，不要引用外部声明的对象，在 map 算子内部创建对象。
* 如果必须引用`map`算子外部的对象，那么就让这个对象在 executor 上初始化，即在 local execution context 中创建对象，不要在类加载的时候就初始化对象，而是在 executor 真正去执行的时候去初始化对象。例如创建一个 redis 的 client，该对象就没有实现`Serializable`接口。

```scala
/**
 *  第一种：使用 rdd.mapPartitions
 *  一个partition就对应着一个task，一个分区中的数据共用一个对象
 */
resultRDD.mapPartitions {
  partition => {
    // create the connection in the context of the mapPartition operation
    val jedis = new Jedis("10.4.121.202", 6379)
    val res = partition.map {
      case (key: String, nameAndIP: String) =>
        jedis.hget(key, nameAndIP)
    }
    jedis.close
    res
  }
}
/**
 * 第二种：使用 @transient lazy 
 * 被 @transient 声明的对象不会被序列化，lazy 声明为对象被调用的时候才会创建对象
 * 这样在 executor 真正使用这个对象的时候才会执行创建操作，相当于为每个 executor
 * 都创建了一个连接对象(jedis)。
 */
object RedisUtil {
  // 当task被分发到executor上执行的时候才会创建对象，没有被创建的对象在分发时不需要被序列化。
  @transient lazy val redis: Jedis = new Jedis("10.4.121.202", 6379)
}
// main
object LogAnalyseAndStatistics {
  def main(args: Array[String]): Unit = {
    resultRDD = ...
    resultRDD.map {
        case (key: String, nameAndIP: String) =>
          // 相当于引用一个外部的实现了Serializable的RedisUtil的闭包
          RedisUtil.redis.hget(key, nameAndIP)
    }
  }
}
```

* 同理，如果是引用了 class 的成员函数导致整个 class 需要支持序列化，可以用`@transient`声明该成员不需要被序列化。
* 让不能被序列化的类`extends Serializable`接口，然后向`KryoSerializer`注册类，并声明该类的序列化方式。

```scala
SparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer")  
SparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable]))
```

* 如果不支持序列化的类不是你自定义的，是第三方包的类，可以将该类的对象放在广播变量中，广播变量会将该变量变成只读的变量，然后缓存在每一台机器(executor)节点上，而非每个Task保存一份拷贝，这样也可以避免Task not serializable的问题

```scala
val sparkContext = sparkSession.sparkContext
val jedisClusterNodes = new HashSet[HostAndPort]()
jedisClusterNodes.add(new HostAndPort("10.4.121.202", 6379))
...
val broadcastJedisCluster = sparkContext.broadcast(new JedisCluster(jedisClusterNodes))
```

## java.io.NotSerializableException: org.apache.spark.SparkContext
sparkContext、sparkConf、sparkSession都是不能被序列化的对象，所以他们不能出现在map等算子中。

## object not serializable(class:org.apache.hadoop.hbase.io.ImmutableBytesWritable)
Spark操作HBase返回的是`RDD[ImmutableWritable,Result]`类型，当在集群上对此RDD进行操作的时候。（比如`join`、`repartition`等进行shuffle），就会产生此异常，因为`org.apache.hadoop.hbase.io.ImmutableBytesWritable`和`org.apache.hadoop.hbase.client.Result`并没有实现`java.io.Serializable`接口

解决方式：

方式一：
```scala
// 向Spark注册，声明ImmutableWritable类需要使用KryoSerializer进行序列化
SparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer")  
SparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable]))
```
方式二：
将ImmutableWritable转换成其他可序列化的类，将其中的数据抽取出来放在可以序列化的类中，比如String或者数组

## java.lang.IllegalArgumentException: requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually org.apache.spark.mllib.linalg.VectorUDT@f71b0bce.
解决办法：
```scala
import org.apache.spark.mllib.linalg.Vectors
// 换成
import org.apache.spark.ml.linalg.Vectors
```
## java.lang.OutOfMemoryError: PermGen space

永生代内存溢出

解决方法：
在IDEA的run configuration中添加VM options  -XX:PermSize=128m

原因：
PermGen space用于存放Class和Meta的信息,Class在被 Load的时候被放入PermGen space区域，它和和存放Instance的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS的话,就很可能
出现PermGen space错误。这种错误常见在web服务器对JSP进行pre compile的时候。

改正方法：
```shell
-Xms256m -Xmx256m -XX:MaxNewSize=256m -XX:MaxPermSize=256m 
```

## org.apache.spark.shuffle.FetchFailedException
### 原因
首先了解下 shuffle 操作，shuffle 分为`shuffle read`和`shuffle write`两部分。 

`shuffle read`的分区数是由 spark 提供的一些配置参数控制。如果这个参数值设置的很小，而`shuffle read`的数据量很大，那么 task 需要处理的数据量就会很大，`JVM`内存使用过多会导致`JVM`长时间 GC 或者直接崩溃，executor 无法及时发送响应心跳，当超过默认的`spark.network.timeout=120s`，就会移除 executor，`Failed to connect to hadoop1/10.53.187.33:43791`就证明已经`lost executor`，这样就会导致`lost task`，从而`shuffle read/fetch`失败。当发生`lost task`，spark 会 retry，将失败的任务提交到另一个正常的 executor，但是每个 executor 的资源都是一样的，同样没有办法正常执行完该 task，这样所有的 executor 最终都会超时被移除。

`shuffle write`的分区数则由`shuffle read`的 RDD 分区数控制，`shuffle write`将计算的中间结果按某种规则临时放到各个 executor 所在的本地磁盘上。

### 解决方法
#### 减少/避免 shuffle 数据。
1. 只操作相关字段而不是使用全部字段来减少`shuffle`数据量
2. 使用`map side join`或者`broadcast join`来避免`shuffle`。

#### 减少单个 task 数据量
1. 对于 spark sql、DataFrame 的`join`、`group by`、`reduce by`等`shuffle`操作，使用`spark.sql.shuffle.partitions`根据数据量和计算复杂度适量增大分区数(默认是`200`)。
2. 对于 RDD 的`join`、`group by`、`reduce by`等`shuffle`操作，使用`spark.default.parallelism`控制`shuffle read`和`reduce`的分区数(默认为运行任务`cores`的总数)，官方建议设置为运行任务`cores`的总数的2～3倍。
3. 减少数据倾斜。过滤空值、对与某个`key`数据量特别大数据，考虑单独处理、或者改变数据的分区策略。

#### 增加 executor 内存
通过`spark.executor.memory`/`--executor-memory`适当提高 executor 的内存。

#### 增加超时检测时间
增大`spark.network.timeout`，允许有更多的时间去等待心跳响应。
