---
layout: post
title: Ozone
tag: Hadoop
---

两套集群，一套为刚搭建的 Ozone，一套为已有的 Hadoop 集群，进行对接测试，让 Hadoop 使用 Ozone 对象存储文件系统作为后端存储。

## Preparation
### Install Ozone
```shell
$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/ozone/1.1.0/ozone-1.1.0.tar.gz
$ tar -zxvf ozone-1.1.0.tar.gz
# 生成配置模版 ozone-site.xml
$ ozone genconf $OZONE_HOME/etc/hadoop 
$ vim $OZONE_HOME/etc/hadoop/ozone-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<configuration>
    <property>
        <name>ozone.om.address</name>
        <value>0.0.0.0:9862</value>
        <tag>OM, REQUIRED</tag>
        <description>The address of the Ozone OM service. This allows clients to discoverthe address of the OM.</description>
    </property>
    <property>
        <name>ozone.om.db.dirs</name>
	    <value>/data/ozone/omdb</value>
    </property>
    <property>
        <name>ozone.metadata.dirs</name>
	    <value>/data/ozone/metadata</value>
        <tag>OZONE, OM, SCM, CONTAINER, STORAGE, REQUIRED</tag>
        <description>
      	  This setting is the fallback location for SCM, OM, Recon and DataNodes
      	  to store their metadata. This setting may be used only in test/PoC
      	  clusters to simplify configuration.

      	  For production clusters or any time you care about performance, it is
      	  recommended that ozone.om.db.dirs, ozone.scm.db.dirs and
      	  dfs.container.ratis.datanode.storage.dir be configured separately.
        </description>
    </property>
    <property>
        <name>ozone.scm.datanode.id.dir</name>
        <value>/data/ozone/metadata/datanode</value>
    </property>
    <property>
        <name>ozone.scm.client.address</name>
        <value>0.0.0.0:9860</value>
        <tag>OZONE, SCM, REQUIRED</tag>
        <description>
      	  The address of the Ozone SCM client service. This is a required setting.

      	  It is a string in the host:port format. The port number is optional
      	  and defaults to 9860.
    	</description>
    </property>
    <property>
        <name>ozone.scm.names</name>
        <value>localhost</value>
        <tag>OZONE, REQUIRED</tag>
        <description>
      	  The value of this property is a set of DNS | DNS:PORT | IPAddress | IP:PORT.
	      Written as a comma separated string. e.g. scm1,scm2:8020, 7.7.7.7:7777.
      	  This property allows datanodes to discover where SCM is, so that
      	  datanodes can send heartbeat to SCM.
        </description>
    </property>
    <property>
        <name>ozone.replication</name>
	    <value>1</value>
    </property>
    <property>
        <name>ozone.replication.type</name>
        <value>CHAINED</value>
        <tag>OZONE, CLIENT</tag>
        <description>
          Default replication type to be used while writing key into ozone. The
          value can be specified when writing the key, default is used when
          nothing is specified. Supported values: RATIS, STAND_ALONE and CHAINED.
        </description>
    </property>
    <property>
        <name>hdds.scm.safemode.enabled</name>
        <value>true</value>
        <tag>HDDS,SCM,OPERATION</tag>
        <description>Boolean value to enable or disable SCM safe mode.</description>
    </property>
</configuration>
```

```shell
$ ozone scm --init
# Start SCM
$ ozone --daemon start scm
```
>SCM UI: http://localhost:9876，由参数`ozone.scm.http-address`控制

```shell
$ ozone om --init
# Start OM
$ ozone --daemon start om
```
>OM UI: http://localhost:9874，由参数`ozone.om.http-address`控制

```shell
# Execute on every datanode
$ ozone --daemon start datanode

# Create a volume and bucket if you use o3fs scheme.
$ ozone sh volume create /bigdata
$ ozone sh bucket create /bigdata/hdfs

# Upload to key `test.txt` to /bigdata/hdfs
$ ozone sh key put /bigdata/hdfs/test.txt test.txt
# Download key `test.txt` from /bigdata/hdfs
$ ozone sh key get /volume/bucket/test.txt ./
```

### Modify Hadoop
```shell
$ vim $HADOOP_HOME/etc/hadoop/core-site.xml

<!-- o3fs configuration -->
<property>
    <name>fs.defaultFS</name>
    <!-- hdfs namenode service -->
    <!-- <value>hdfs://localhost:9000</value> -->
    
    <!-- o3fs://bucket.volume -->
    <value>o3fs://hdfs.bigdata</value>
</property>
<property>
    <name>fs.AbstractFileSystem.o3fs.impl</name>
    <value>org.apache.hadoop.fs.ozone.OzFs</value>
</property>
<property>
    <name>fs.o3fs.impl</name>
    <value>org.apache.hadoop.fs.ozone.OzoneFileSystem</value>
</property>


<!-- ofs configuration -->
<property>
    <name>fs.ofs.impl</name>
    <value>org.apache.hadoop.fs.ozone.RootedOzoneFileSystem</value>
</property>
<property>
    <name>fs.defaultFS</name>
    <value>ofs://om-host.example.com/</value>
</property>

# 添加依赖，Hadoop 3.x 改成 hadoop3
export HADOOP_CLASSPATH=$OZONE_HOME/share/ozone/lib/hadoop-ozone-filesystem-hadoop2-*.jar:$HADOOP_CLASSPATH
# 或者
$ cp $OZONE_HOME/share/ozone/lib/hadoop-ozone-filesystem-hadoop2-*.jar $HADOOP_HOME/share/hadoop/common/

# 将 ozone-site.xml 拷贝到 HADOOP_CONF_DIR
$ cp $OZONE_HOME/etc/hadoop/ozone-site.xml $HADOOP_HOME/etc/hadoop

# 重启 HDFS 服务
$ $HADOOP_HOM/sbin/start-dfs.sh

$ hadoop fs -ls /
```

>Note: Ozone supports two scheme: `o3fs://` and `ofs://`.  The biggest difference between the o3fs and ofs，is that o3fs supports operations only at a single bucket, while ofs supports operations across all volumes and buckets and provides a full view of all the volume/buckets.


## Recon
```shell
$ ozone --daemon start recon
```

> Recon UI: http://localhost:9888

## S3 compatible REST gateway
```shell
$ ozone --daemon start s3g
```
