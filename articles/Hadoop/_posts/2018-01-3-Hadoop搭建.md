---
layout: post
title: Hadoop搭建
tag: Hadoop
---

## 搭建前的准备
　　本次搭建Hadoop版本是hadoop3.0.0-release，[点击下载安装包](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.0.0/hadoop-3.0.0.tar.gz)。

　　伪分布式集群搭建参考，[官方文档](http://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-common/SingleCluster.html)

## 搭建过程
### 新建hadoop用户
```shell
useradd hadoop
# 为hadoop用户分配sudo权限
visudo
# 添加如下内容
hadoop ALL=(ALL)       NOPASSWD: ALL
# 设置hadoop密码
passwd hadoop
```
### 将下载好的hadoop-3.0.0.tar.gz包解压到`/home/hadoop/`。
```shell
su - hadoop 
tar -zxvf hadoop-3.0.0.tar.gz -C /home/hadoop/
```
### 配置环境变量。
```shell
vim ~/.bash_profile
export HADOOP_HOME=/home/hadoop/hadoop-3.0.0
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
source ~/.bash_profile
# 配置Hadoop环境变量
vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/java/jdk1.8.0_111/
# 注：mac查找JAVA_HOME的方法
# === 方法1 ===
which java
/usr/bin/java
ls -l /usr/bin/java
lrwxr-xr-x  1 root  wheel  74 12 19 19:56 /usr/bin/java -> /System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java
cd /System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/
./java_home -V
# 显示如下信息：
Matching Java Virtual Machines (1):
    1.8.0_144, x86_64:	"Java SE 8"	/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home
# 下面这一行就是JAVA_HOME
/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home
# === 方法2 ===
/usr/libexec/java_home -V
```
### 修改 core-site.xml
`vim $HADOOP_HOME/etc/hadoop/core-site.xml`

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```
### 修改 hdfs-site.xml
`vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml`

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```
### 配置ssh。
```shell
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
# 测试
ssh localhost
```
### 格式化FileSystem并启动
Hadoop守护进程日志输出地址配置在`$HADOOP_LOG_DIR`，默认是`$HADOOP_HOME/logs`。

```shell
# 格式化
hdfs namenode -format
# 启动hdfs
start-dfs.sh
# 在浏览器查看NameNode启动情况
http://localhost:9870/
# jps查看进程启动情况
18284 NameNode
18362 DataNode
18460 SecondaryNameNode
```


### 配置 mapred-site.xml
`vim $HADOOP_HOME/etc/hadoop/mapred-site.xml`

```shell
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```
### 配置 yarn-site.xml
`vim $HADOOP_HOME/etc/hadoop/yarn-site.xml`

```shell
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```
### 启动yarn
```shell
# 启动
sbin/start-yarn.sh
# 浏览器查看ResourceManager启动情况 
http://localhost:8088/
# jps 查看进程
14481 NodeManager
```
## 安装中出现的问题
### 在更换版本时 Incompatible clusterIDs
```console
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-hadoop/dfs/data: namenode clusterID = CID-2814bb5f-3867-4f19-bc67-d0f3c3ca784e; datanode clusterID = CID-1bded296-4e6e-473c-91b6-6d3174f98926
```
很明显意思是namenode的clusterID和dataNode的clusterID不一致导致的。

```shell
# 关掉dfs
stop-dfs.sh
# 删掉有冲突的文件
rm -rf /tmp/hadoop-hadoop/
# 重新格式化即可
hdfs namenode -format
```

### Unable to load native-hadoop ...
```console
WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... 
using builtin-java classes where applicable
```
参考[这篇博客](http://blog.csdn.net/l1028386804/article/details/51538611)

### InconsistentFSStateException
```console
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: 
Directory /private/tmp/hadoop-despacito/dfs/name is in an inconsistent state: 
storage directory does not exist or is not accessible.
```

在启动Hadoop的时候会在`${hadoop.tmp.dir}`文件夹生成一堆文件，`${hadoop.tmp.dir}`属性配置在`conf/core-site.xml`中，默认是下面的样子。

```xml
<property>
   <name>hadoop.tmp.dir</name>
   <value>/private/tmp/hadoop-${user.name}</value>
   <description>A base for other temporary directories.</description>
</property>
```

如果tmp文件夹中的内容会被清理，Hadoop启动的时候不会恢复`${hadoop.tmp.dir}/dfs/name`这个文件夹，因此需要执行`hdfs namenode -format`重新生成这个文件夹就可以了。

