---
layout: post
title: 朴素贝叶斯
tag: Machine Learning
---
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

## 贝叶斯定理
设\\( X \\)是代表一条数据(一个对象)，由\\( n \\)个属性\\( A_1,A_2,\ldots,A_n \\)构成；\\( H \\)为某种假设，如数据\\( X \\)属于某个特定的类\\( C \\)。\\( P(H\|X)\\) 是在已知\\( X \\)的几个属性下，\\( X \\)属于某个类\\( C \\)的概率。贝叶斯定理如下：
\\[ P(H\|X) =  \frac{P(X\|H)P(H)}{P(X)} \\]
其中，\\( P(H\|X)\\)是在条件\\(X\\)下，\\(H\\)的后验概率，\\( P(H)\\)是\\(H\\)的先验概率。
## 朴素贝叶斯(Naive Bayesian)
1. 设\\(D\\)是包含数据和其所属类的集合。每条数据由n维属性向量\\(X = \{ x_1,x_2,\ldots,x_n\}\\)表示。
2. 假设数据集\\(D\\)有m个类\\( C_1,C_2,\ldots,C_m \\)，用朴素贝叶斯预测某一条数据\\(X\\)属于哪一类就变成了概率问题，即属于哪一类的概率最大。
\\[ P(C_i\|X) =  \frac{P(X\|C_i)P(C_i)}{P(X)} \\]
由于\\(P(X)\\)对于所有类为常数，所以只需要求出最大的\\(P(X\|C_i)P(C_i)\\)。
3. 如果类的先验概率未知，通常假定属于哪个类是等概率的，即\\(P(C_1)=P(C_2)=\cdots=P(C_m)\\)，否则可以用\\(P(C_i)=\|C_i\|/\|D\|\\)来估计。
4. 在属性很多的情况下，计算\\( P(X\|C_i) \\)的开销可能会非常大，为了降低开销，可以做**类条件独立**的朴素假定。因此有如下等式。
\\[ P(X\|C_i) = \prod_{k=1}^n P(x_k\|C_i) = p(x_1\|C_i)p(x_2\|C_i) \cdots p(x_n\|C_i) \\]
对于数据的每个属性\\(A_k\\)，考察其值是离散的还是连续的，其中\\(x_k\\)代表数据\\(X\\)的\\(A_k\\)属性的值。
* 如果属性\\(A_k\\)的值是离散的，则\\( P(x_i\|C_k) \\)为数据集\\(D\\)中属于\\(C_i\\)类且属性\\(A_k\\)的值是\\(x_k\\)的数据的数量除以属于\\(C_i\\)类数据的数量。
* 如果属性\\(A_k\\)的值是连续的，通常假定此连续的属性值是服从均值为\\(\mu\\)，标准差为\\( \sigma \\)的高斯分布，由下式定义
\\[ g(x,\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\]
\\[ P(x_k\|C_i)=g(x_k,\mu_{C_i},\sigma_{C_i}) \\]
其中\\(\mu_{C_i}\\)和\\(\sigma_{C_i}\\)是属于\\(C_i\\)类数据属性\\(X_k\\)均值和标准差。
5. 对于每个类\\(C_i\\) 计算\\( P(X\|C_i)P(C_i) \\)最后得出最大的\\(C_i\\)就是\\(X\\)的预测所属类

## 拉普拉斯校准
如果对类\\(C_1\\)的数据，其属性值\\(x_1=1\\)的数量为0，即某一项\\(P(x_1\|C_i)=0\\)，就会导致P(X\|C_i)=0，不管其它后验概率\\(P(x_{2\ldots n}\|C_i)\\)是多少。为了避免这种情况发生，使用拉普拉斯校准:
> 如果对q个计数都加上1，则必须记住在用于计算概率的对应分母上加上q。

例如：假设在某数据集\\(D\\)中，属于类\\(C_1\\)(有购买计算机行为)的数据有10000条，其中对于属性\\(X_1\\)(收入等级)，收入低的数据有0条，收入中等的数据有8000条，收入高的数据有2000条。

不使用拉普拉斯校准的情况下，这些事件发生的概率为0，0.8，0.2。

使用拉普拉斯校准分别为\\(\frac{0+1}{10000+3}\\)，\\(\frac{8000+1}{10000+3}\\)，\\(\frac{2000+1}{10000+3}\\)。

校准后的概率与未校准的概率很接近，而且避免了0概率值。