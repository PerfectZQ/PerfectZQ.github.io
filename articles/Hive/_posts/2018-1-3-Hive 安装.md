---
layout: post
title: Hive 安装
tag: Hive
---

## 搭建前的准备
首先得搭建好Hadoop，可以参考[之前的博客](https://arch-long.cn/articles/hadoop/Hadoop%E6%90%AD%E5%BB%BA.html)

Hive版本：apache-hive-2.3.2-bin.tar.gz。[点击下载](http://mirrors.tuna.tsinghua.edu.cn/apache/hive/)。

官方手册[清单(目录)](https://cwiki.apache.org/confluence/display/Hive/Home)

官方手册[开始Hive安装和配置](https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-InstallationandConfiguration)

官方手册[LanguageManual 语法](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)

参考博客[Hive的几种内置服务](http://blog.csdn.net/gamer_gyt/article/details/52062460)

参考博客[Hive-Metastore及其配置管理](http://blog.csdn.net/skywalker_only/article/details/26219619)
    
参考博客[Hive metastore三种配置方式](http://blog.csdn.net/reesun/article/details/8556078)

## 安装过程

### 解压
```shell
tar -xzvf hive-2.3.2.tar.gz -C /home/hadoop/
```
### 配置环境变量
PATH 环境变量中必须有 HADOOP_HOME
```shell
vim ~/.bash_profile
export HIVE_HOME=/home/hadoop/apache-hive-2.3.2
export PATH=$HIVE_HOME/bin:$PATH
source ~/.bash_profile
```
### 安装 mysql
如果不使用内嵌的 derby 管理 Metastore，可以配置 mysql 来管理 Metastore。 

参考[安装Mysql](https://arch-long.cn/articles/other/Mysql-Linux.html)

```shell
# root用户登录 -h 指定 hostname，默认localhost
mysql -u root -p
# 创建一个新数据库实例，在mysql shell中';'是必须的
mysql> create database hive;
mysql> grant all privileges on hive.* to hive@"%" identified by 'hive';
```
添加 mysql 驱动包，去 mvn [下载](http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar) mysql 驱动包
```shell
mv mysql-connector-java-5.1.38.jar $HIVE_HOME/lib
```

### 修改配置文件

你可以在`conf/hive-env.sh`里添加额外的环境变量。

```shell
cd $HIVE_HOME/conf/
cp hive-env.sh.template hive-env.sh
```

Hive 配置是Hadoop之上的一个覆盖，它默认继承了Hadoop配置变量，并默认从`conf/hive-default.xml`中读取配置项，用户可以通过`conf/hive-site.xml`覆盖默认的配置。

Hive 配置文件夹可以通过`$HIVE_CONF_DIR`来修改，默认是`$HIVE_HOME/conf/`

```shell
# === 方式1 ===
cp hive-default.xml.template hive-default.xml
# 但是这样做会出现问题：
# java.lang.IllegalArgumentException:java.net.URISyntaxException: 
# Relative path in absolute URI:${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
# 原因: Hive 找不到 ${system:java.io.tmpdir}，${system:user.name}
# 解决方案:
# 创建本地临时文件夹
mkdir /home/hadoop/hive-2.3.2/temp
vim hive-default.xml
# 将所有 ${system:java.io.tmpdir} 替换成 /home/hadoop/hive-2.3.2/temp
:%s/\${system:java.io.tmpdir}/\/home\/hadoop\/hive-2.3.2\/temp/g
# 将所有 ${system:user.name} 改成 ${user.name}
:%s/\${system:user.name}/${user.name}/g
# === 方式2 ===
# 新建新的 hive-site.xml，当然需要包含 xml 命名空间和 configuration 标签
vim hive-site.xml
```

方式1需要修改，方式2需要添加如下内容

```xml
<configuration>
    <!-- 注意：下面配置的都是 hdfs 路径 -->
    <property>
        <!-- 
        指定Hive作业的HDFS根目录，全部写入权限(733)。
        对于每一个连接的用户会创建一个HDFS临时目录，
        路径是 ${hive.exec.scratchdir}/username，
        并指定权限 ${hive.scratch.dir.permission} (700)
        -->
        <name>hive.exec.scratchdir</name>
        <value>/tmp/hive</value>
    </property>
    <property>
        <!-- 指定仓库默认数据库的位置 -->
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
    <property>
        <!-- Hive运行时结构化日志文件的位置 -->
        <name>hive.querylog.location</name>
        <value>/user/hive/log</value>
    </property>
    <!-- 配置 Metastore 官网默认配置是 derby，配置 mysql 需要改成下面的样子 -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <!-- hive 是 mysql 数据库的名称 -->
        <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
    </property>
    <property>
        <!-- 数据库驱动类 -->
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <!-- 数据库用户名 -->
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
    </property>
    <property>
        <!-- 数据库密码 -->
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>hive</value>
    </property>
</configuration>
```
### 在hdfs创建基础目录并赋权
如果hive拥有hdfs根目录的写权限就不需要下面的操作了，他自己会生成。

```shell
# hive 的数据结构目录
bin/hadoop fs -mkdir -p /user/hive/warehouse  
bin/hadoop fs -mkdir -p /tmp/hive
bin/hadoop fs -mkdir -p /user/hive/log  
bin/hadoop fs -chmod -R 777 /user/hive/warehouse  
bin/hadoop fs -chmod -R 777 /tmp/hive
bin/hadoop fs -chmod -R 777 /user/hive/log  
```

### 初始化
使用 derby 进行初始化

```shell
schematool -dbType derby -initSchema
# 在当前目录下 会看到一个文件 derby.log 和一个文件夹 metastore_db
```

使用 mysql 进行初始化

```shell
schematool -dbType mysql -initSchema
```

## Hive 的内置服务

```shell
# 查看 hive 内置服务
hive --service help
```

显示如下：

```console
Usage ./hive <parameters> --service serviceName <service parameters>
# hive的所有服务/组件
Service List: beeline cleardanglingscratchdir cli hbaseimport hbaseschematool help hiveburninclient hiveserver2 hplsql jar lineage llap llapdump llapstatus metastore metatool orcfiledump rcfilecat schemaTool version 
Parameters parsed:
  --auxpath : Auxiliary jars 
  --config : Hive configuration directory
  --service : Starts specific service/component. cli is default
Parameters used:
  HADOOP_HOME or HADOOP_PREFIX : Hadoop install directory
  HIVE_OPT : Hive options
# 查看某个服务/组件的详细用法
For help on a particular service:
  ./hive --service serviceName --help
Debug help:  ./hive --debug --help
```

下面简单介绍几个服务：
* cli：是 Hive 的命令行界面，用的比较多，是默认服务，直接可以在命令行里使用。
* hiveserver：这个可以让Hive以提供Thrift服务的服务器形式来运行，可以允许许多个不同语言编写的客户端进行通信，使用需要启动HiveServer服务以和客户端联系，我们可以通过设置HIVE_PORT环境变量来设置服务器所监听的端口，在默认情况下，端口号为10000。这个可以通过以下方式来启动Hiverserver：`bin/hive --service hiveserver -p 10002`
* HWI：其实就是hive web interface的缩写它是hive的web借口，是hive cli的一个web替代方案(该组件从Hive 2.2.0 开始被移除了)
* jar：与 `hadoop jar` 等价的 Hive 接口，这是运行类路径中同时包含 Hadoop 和 Hive 类的 Java 应用程序的简便方式
* metastore：在默认的情况下，metastore和hive服务运行在同一个进程中，使用这个服务，可以让metastore作为一个单独的进程运行，我们可以通过METASTOE——PORT来指定监听的端口号

## 启动
Hive 的启用方式大概可以分为三类：CLI(Command Line Interface)、HWI(Hive Web Interface)、HiveServer
### CLI
启动 hive-cli 

```shell
hive
```

开启hive的log4j日志，运行hive cli 的日志信息会在配置的目录下生成

```shell
mkdir /home/hadoop/apache-hive-2.3.2/logs
cd $HIVE_HOME/conf
cp hive-log4j2.properties.template hive-log4j2.properties
vim hive-log4j2.properties
property.hive.log.dir = /home/hadoop/apache-hive-2.3.2/logs
```


### HWI
启动 HWI，用于通过浏览器来访问hive，浏览器访问地址是：127.0.0.1:9999/hwi。注意：该组件在Hive2.2.0开始被移除了。

```shell
bin/hive --service hwi &
```
### HiveServer
启动 HiveServer2，用java、python等程序实现通过 jdbc 等驱动访问 hive 就需要这种启动方式。

```shell
hiveserver2 &
# 默认地址:`jdbc:hive2://localhost:10000`
```

使用beeline 连接 hiveserver2

```shell
beeline -u jdbc:hive2://localhost:10000/
```

如果出现以下异常：

```console
Error: 
Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000/: 
Failed to open new session: 
java.lang.RuntimeException: 
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): 
User: despacito is not allowed to impersonate anonymous (state=08S01,code=0)
```

是因为hadoop引入了一个安全伪装机制，使得hadoop不允许上层系统直接将实际用户传递到hadoop层，而是将实际用户传递给一个超级代理，由此代理在hadoop上执行操作，避免任意客户端随意操作hadoop。

解决方法：在hadoop的配置文件core-site.xml增加如下配置，并重启hdfs。

```xml
<property>
    <!-- 需要将 xxx 换成你的超级代理的用户名，说明请求将由 xxx 用户代执行 -->
    <name>hadoop.proxyuser.xxx.hosts</name>
    <!-- 哪些 hosts 的请求要走超级代理，逗号分割，* 代表所有-->
    <value>*</value>
</property>
<property>
    <!-- 同上 -->
    <name>hadoop.proxyuser.xxx.groups</name>
    <!-- 哪些 gorup 的请求要走超级代理，同上 -->
    <value>*</value>
</property>
```

在`$HIVE_HOME/conf/hive-site.xml`中配置Hiveserver2的相关配置。

```shell
# 最小工作线程数，默认为5。
hive.server2.thrift.min.worker.threads
# 最大工作线程数，默认为500。  
hive.server2.thrift.max.worker.threads
# TCP 的监听端口，默认为10000。  
hive.server2.thrift.port
# TCP绑定的主机，默认为localhost
hive.server2.thrift.bind.host
# 默认值为binary（TCP），可选值HTTP。  
hive.server2.transport.mode
# HTTP的监听端口，默认值为10001。
hive.server2.thrift.http.port
# 服务的端点名称，默认为 cliservice。
hive.server2.thrift.http.path
# 服务池中的最小工作线程，默认为5。
hive.server2.thrift.http.min.worker.threads
# 服务池中的最大工作线程，默认为500。
hive.server2.thrift.http.max.worker.threads
# 设置 impersonation，默认是 true
# 如果为 true，hive server 会以提交用户的身份去执行语句。
# 如果设置为 false，则会以起 hive server daemon 的 admin user 来执行语句。
hive.server2.enable.doAs
```