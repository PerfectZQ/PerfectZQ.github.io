---
layout: post
title: Hive安装
tag: Hive
---

## 搭建前的准备
　　首先得搭建好Hadoop，可以参考[之前的博客](https://arch-long.cn/articles/hadoop/Hadoop%E6%90%AD%E5%BB%BA.html)

　　Hive版本：apache-hive-2.3.2-bin.tar.gz。[点击下载](http://mirrors.tuna.tsinghua.edu.cn/apache/hive/)。

　　参考[官方手册](https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-InstallationandConfiguration)

　　参考博客[Hive的几种内置服务](http://blog.csdn.net/gamer_gyt/article/details/52062460)

　　参考博客[Hive-Metastore及其配置管理](http://blog.csdn.net/skywalker_only/article/details/26219619)
    
　　参考博客[Hive metastore三种配置方式](http://blog.csdn.net/reesun/article/details/8556078)
## 安装过程

### 解压
```shell
tar -xzvf hive-2.3.2.tar.gz -C /home/hadoop/
```
### 配置环境变量
　　PATH 环境变量中必须有 HADOOP_HOME
```shell
vim ~/.bash_profile
export HIVE_HOME=/home/hadoop/apache-hive-2.3.2
export PATH=$HIVE_HOME/bin:$PATH
source ~/.bash_profile
```
### 安装 mysql
如果不使用内嵌的 derby 管理 Metastore，可以配置 mysql 来管理 Metastore。 

参考[安装mysql](https://arch-long.cn/articles/other/Mysql-Linux%E5%AE%89%E8%A3%85.html)

```shell
# root用户登录 -h 指定 hostname，默认localhost
mysql -u root -p
# 创建一个新数据库实例，在mysql shell中';'是必须的
mysql> create database hive;
mysql> grant all privileges on hive.* to hive@"%" identified by 'hive';
```
添加 mysql 驱动包，去 mvn [下载](http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar) mysql 驱动包
```shell
mv mysql-connector-java-5.1.38.jar $HIVE_HOME/lib
```

### 修改配置文件
你可以在`conf/hive-env.sh`里添加额外的环境变量。

```shell
cd $HIVE_HOME/conf/
cp hive-env.sh.template hive-env.sh
```

Hive 默认从`conf/hive-default.xml`中读取配置项，用户可以通过`conf/hive-site.xml`覆盖默认的配置。

Hive 配置文件夹可以通过`$HIVE_CONF_DIR`来修改，默认是`$HIVE_HOME/conf/`

```shell
# === 方式1 ===
cp hive-default.xml.template hive-default.xml
# 但是这样做会出现问题：
# java.lang.IllegalArgumentException:java.net.URISyntaxException: 
# Relative path in absolute URI:${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
# 原因: Hive 找不到 ${system:java.io.tmpdir}，${system:user.name}
# 解决方案:
# 创建本地临时文件夹
mkdir /home/hadoop/hive-2.3.2/temp
vim hive-default.xml
# 将所有 ${system:java.io.tmpdir} 替换成 /home/hadoop/hive-2.3.2/temp
:%s/\${system:java.io.tmpdir}/\/home\/hadoop\/hive-2.3.2\/temp/g
# 将所有 ${system:user.name} 改成 ${user.name}
:%s/\${system:user.name}/${user.name}/g
# === 方式2 ===
# 新建新的 hive-site.xml，当然需要包含 xml 命名空间和 configuration 标签
vim hive-site.xml
```

　　方式1需要修改，方式2需要添加如下内容

```xml
<configuration>
    <!-- 注意：下面配置的都是 hdfs 路径 -->
    <property>
        <!-- 
        指定Hive作业的HDFS根目录，全部写入权限(733)。
        对于每一个连接的用户会创建一个HDFS临时目录，
        路径是 ${hive.exec.scratchdir}/username，
        并指定权限 ${hive.scratch.dir.permission} (700)
        -->
        <name>hive.exec.scratchdir</name>
        <value>/tmp/hive</value>
    </property>
    <property>
        <!-- 指定仓库默认数据库的位置 -->
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
    <property>
        <!-- Hive运行时结构化日志文件的位置 -->
        <name>hive.querylog.location</name>
        <value>/user/hive/log</value>
    </property>
    <!-- 配置 Metastore 官网默认配置是 derby，配置 mysql 需要改成下面的样子 -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <!-- hive 是 mysql 数据库的名称 -->
        <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
    </property>
    <property>
        <!-- 数据库驱动类 -->
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <!-- 数据库用户名 -->
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
    </property>
    <property>
        <!-- 数据库密码 -->
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>hive</value>
    </property>
</configuration>
```
### 日志配置
```shell
cd $HIVE_HOME/conf
cp hive-log4j2.properties.template hive-log4j2.properties
vim hive-log4j2.properties
property.hive.log.dir = /home/hadoop/apache-hive-2.3.2/logs
```
### 在hdfs创建基础目录并赋权
如果hive拥有hdfs根目录的写权限就不需要下面的操作了，他自己会生成。
```shell
# hive 的数据结构目录
bin/hadoop fs -mkdir -p /user/hive/warehouse  
bin/hadoop fs -mkdir -p /tmp/hive
bin/hadoop fs -mkdir -p /user/hive/log  
bin/hadoop fs -chmod -R 777 /user/hive/warehouse  
bin/hadoop fs -chmod -R 777 /tmp/hive
bin/hadoop fs -chmod -R 777 /user/hive/log  
```

### 初始化
使用 derby 进行初始化
```shell
schematool -dbType derby -initSchema
# 在当前目录下 会看到一个文件 derby.log 和一个文件夹 metastore_db
```
使用 mysql 进行初始化
```shell
schematool -dbType mysql -initSchema
```
## Hive 的内置服务
```shell
# 查看 hive 内置服务
hive --service help
```
显示如下：
```console
Usage ./hive <parameters> --service serviceName <service parameters>
# hive的所有服务/组件
Service List: beeline cleardanglingscratchdir cli hbaseimport hbaseschematool help hiveburninclient hiveserver2 hplsql jar lineage llap llapdump llapstatus metastore metatool orcfiledump rcfilecat schemaTool version 
Parameters parsed:
  --auxpath : Auxiliary jars 
  --config : Hive configuration directory
  --service : Starts specific service/component. cli is default
Parameters used:
  HADOOP_HOME or HADOOP_PREFIX : Hadoop install directory
  HIVE_OPT : Hive options
# 查看某个服务/组件的详细用法
For help on a particular service:
  ./hive --service serviceName --help
Debug help:  ./hive --debug --help
```
## 启动
Hive 的启动方式大概可以分为三类：CLI(Command Line Interface)、HWI(Hive Web Interface)、HiveServer
### CLI
### HWI
### HiveServer