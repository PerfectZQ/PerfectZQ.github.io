---
layout: post
title: Spark 异常处理记录
tag: Spark
---

# Spark 异常处理记录
### spark程序运行空指针异常的原因
1.嵌套使用了RDD操作，比如在一个RDD map中又对另一个RDD进行了map操作。主要原因在于spark不支持RDD的嵌套操作。

2.在RDD的算子操作中引用了object非原始类型(非int long等简单类型)的成员变量。是由于object的成员变量默认是无法序列化的。解决方法：可以先将成员变量赋值给一个临时变量，然后使用该临时变量即可

3.spark 2.0.0对kryo序列化的依赖有bug，到SPARK_HOME/conf/spark-defaults.conf
将默认： spark.serializer     org.apache.spark.serializer.KryoSerializer
改为： spark.serializer 	  org.apache.spark.serializer.JavaSerializer

### Task not serializable
　　在map等算子内部使用外部定义的函数和变量时，如果这个外部定义的变量不能被序列化时，就会出现这种问题。

　　但是在算子的计算过程中难免需要使用外部变量，要使用这些外部变量，就需要对这些外部变量做好序列化工作。

　　最普遍的情形是：当引用了某个类（绝大多数是当前class或者object）的某个成员变量或者成员函数的时候，将导致这个类的所有的成员都需要支持序列化。即便这个类extends了Serializable，声明支持序列化，如果某些字段(成员对象，Object)不支持序列化，仍然会导致这鞥各类在序列化的时候出现问题。可以将类中不能被序列化的成员变量标注为@transient，表示不对其进行序列化。

　　要解决Task not serializable的问题：
* 在条件允许的情况下，不要引用外部声明的对象，在map算子内部创建对象。
* 让不能被序列化的类extends Serializable接口，然后向KryoSerializer注册类，声明该类的序列化方式。
```
SparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer")  
SparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable]))
```
* 如果不支持序列化的类不是你自定义的，是第三方包的类，可以将该类的对象放在广播变量中，广播变量会将该变量变成只读的变量，然后缓存在每一台机器节点上，而非每个Task保存一份拷贝，这样也可以避免Task not serializable的问题
```
val sparkContext = sparkSession.sparkContext
val jedisClusterNodes = new HashSet[HostAndPort]()
jedisClusterNodes.add(new HostAndPort("10.4.121.202", 6379))
...
val broadcastJedisCluster = sparkContext.broadcast(new JedisCluster(jedisClusterNodes))
```
* 终极大法，如果将对象放在广播变量中出现问题，也可以选择将该对象定义为一个object的成员变量，作为object的成员变量，类似与Java的static成员变量，这样就取消了对类的依赖，从而避免出现Task not serializable的问题
```
object RedisUtil {
  // 成员变量在初始化的时候之调用一次init()方法
  val clients: JedisCluster = init()
  private def init(): JedisCluster = {
    val properties = new Properties()
    properties.load(new FileInputStream("""D:\property files\gd_log_analyse.properties"""))
    // 集群模式需要将 redis.conf中的cluster-enabled置为true，单节点需要置为false，否则会报集群挂掉的异常
    val jedisClusterNodes = new HashSet[HostAndPort]()
    val redisAddress: String = properties.get("redisAddress").toString
    redisAddress
      .split(",")
      .foreach(
        host => {
          val arr = host.split(":")
          println(s"""${arr(0)}:${arr(1)}""")
          jedisClusterNodes.add(new HostAndPort(arr(0), arr(1).toInt))
          //   jedis = new Jedis(arr(0), arr(1).toInt)
        })
    new JedisCluster(jedisClusterNodes)
  }
}

object LogAnalyseAndStatistics {
  def main(args: Array[String]): Unit = {
    resultRDD = ...
    resultRDD.foreach {
       case (username: String, channel: Int, urlRequestStatistics: Map[String, String]) =>
         val map = new mutable.HashMap[String, String]()
         map.put("name", username)
         map.put("channel", channel.toString)
         map ++= urlRequestStatistics
         // 这样就相当于将clients对象定义在了map算子内部
         RedisUtil.clients.hmset(currentTime, map)
       }
    }
}
```
* 同理。如果是引用了class的成员函数导致整个class需要支持序列化，可以通过将此成员函数放在Scala的object中，类似与Java的static，这样就取消了对类对象的依赖。
### java.io.NotSerializableException: org.apache.spark.SparkContext
　　sparkContext、sparkConf、sparkSession都是不能被序列化的对象，所以他们不能出现在map等算子中。
### object not serializable(class:org.apache.hadoop.hbase.io.ImmutableBytesWritable)
　　Spark操作HBase返回的是 RDD[ImmutableWritable,Result]类型，当在集群上对此RDD进行操作的时候。（比如join），就会产生此异常，因为org.apache.hadoop.hbase.io.ImmutableBytesWritable和org.apache.hadoop.hbase.client.Result并没有实现java.io.Serializable接口

解决方式：

方式一：
```
// 手动设置如何序列化ImmutableWritable类
SparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer")  
SparkConf.registerKryoClasses(Array(classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable]))
```
方式二：
将ImmutableWritable转换成其他可序列化的类，将其中的数据抽取出来放在可以序列化的类中，比如String或者数组

### 使用 Spark ML 决策树构建 DataFrame 的时候出现下面的异常：java.lang.IllegalArgumentException: requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually org.apache.spark.mllib.linalg.VectorUDT@f71b0bce.
解决办法：
```
import org.apache.spark.mllib.linalg.Vectors
=>
import org.apache.spark.ml.linalg.Vectors
```
### 运行 Spark ML 决策树的时候出现 java.lang.OutOfMemoryError: PermGen space （永生代的内存溢出）

解决方法：
在IDEA的run configuration中添加VM options  -XX:PermSize=128m

原因：
PermGen space用于存放Class和Meta的信息,Class在被 Load的时候被放入PermGen space区域，它和和存放Instance的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGen space进行清理，所以如果你的APP会LOAD很多CLASS的话,就很可能
出现PermGen space错误。这种错误常见在web服务器对JSP进行pre compile的时候。

改正方法：
```
-Xms256m -Xmx256m -XX:MaxNewSize=256m -XX:MaxPermSize=256m 
```

